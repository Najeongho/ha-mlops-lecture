# ============================================================
# KServe InferenceService - Sklearn Model
# Lab 2-3: KServe 배포
# ============================================================
#
# 사용법:
#   kubectl apply -f sklearn-inferenceservice.yaml -n <namespace>
#
# 참고:
#   - storageUri는 본인의 S3 경로로 변경하세요.
#   - namespace는 본인의 네임스페이스로 변경하세요.
# ============================================================

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: california-model
  labels:
    app: california-model
    app.kubernetes.io/name: california-model
    app.kubernetes.io/part-of: mlops-training
    app.kubernetes.io/component: model-serving
  annotations:
    # Autoscaling 설정
    autoscaling.knative.dev/minScale: "1"
    autoscaling.knative.dev/maxScale: "3"
    autoscaling.knative.dev/target: "10"  # 동시 요청 수
spec:
  predictor:
    # 모델 유형: sklearn
    sklearn:
      # S3 모델 경로 (본인 경로로 변경)
      storageUri: "s3://mlops-training-models/california-model/model"
      
      # 리소스 설정
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
      
      # 런타임 버전 (선택적)
      # runtimeVersion: "1.0.0"
    
    # Pod 설정 (선택적)
    # serviceAccountName: kserve-sa
    # timeout: 60
    # canaryTrafficPercent: 0

---
# Canary 배포 예시
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: california-model-canary
  annotations:
    autoscaling.knative.dev/minScale: "1"
spec:
  predictor:
    # Canary 트래픽 비율 (10%)
    canaryTrafficPercent: 10
    sklearn:
      storageUri: "s3://mlops-training-models/california-model-v2/model"
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
