"""
[Day 3 í”„ë¡œì íŠ¸] ì™„ì„± ì†”ë£¨ì…˜
California Housing E2E MLOps íŒŒì´í”„ë¼ì¸

í‰ê°€ ê¸°ì¤€:
- Pipeline êµ¬ì„± (40ì ): ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì—°ê²°, ì‹¤í–‰ ì„±ê³µ
- MLflow ì—°ë™ (20ì ): ì‹¤í—˜ ì¶”ì , ëª¨ë¸ ë“±ë¡
- Feature Engineering (15ì ): ìµœì†Œ 2ê°œ ì´ìƒ íŒŒìƒ ë³€ìˆ˜
- KServe ë°°í¬ (25ì ): InferenceService ë°°í¬, API í…ŒìŠ¤íŠ¸

í˜„ëŒ€ì˜¤í† ì—ë²„ MLOps êµìœ¡
"""

import os
from kfp import dsl
from kfp.dsl import component, Input, Output, Dataset, Model, Metrics
from typing import NamedTuple


# ============================================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
# ============================================================
USER_NAMESPACE = os.environ.get("USER_NAMESPACE", "kubeflow-user01")
MLFLOW_TRACKING_URI = os.environ.get(
    "MLFLOW_TRACKING_URI", 
    "http://mlflow-server.mlflow-system.svc.cluster.local:5000"
)
S3_BUCKET = os.environ.get("S3_BUCKET", "mlops-training-bucket")
TEAM_NAME = os.environ.get("TEAM_NAME", "team-01")


# ============================================================
# Component 1: ë°ì´í„° ë¡œë“œ
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["pandas==2.0.3", "scikit-learn==1.3.2"]
)
def load_data(output_data: Output[Dataset]):
    """California Housing ë°ì´í„°ì…‹ ë¡œë“œ"""
    from sklearn.datasets import fetch_california_housing
    import pandas as pd
    
    print("=" * 60)
    print("Step 1: ë°ì´í„° ë¡œë“œ")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    housing = fetch_california_housing(as_frame=True)
    df = housing.frame
    
    print(f"ë°ì´í„°ì…‹ í¬ê¸°: {df.shape}")
    print(f"í”¼ì²˜: {list(df.columns)}")
    print(f"\nê¸°ë³¸ í†µê³„:")
    print(df.describe())
    
    # ì €ì¥
    df.to_csv(output_data.path, index=False)
    print(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_data.path}")


# ============================================================
# Component 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (15ì )
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["pandas==2.0.3", "numpy==1.24.3"]
)
def feature_engineering(
    input_data: Input[Dataset],
    output_data: Output[Dataset]
) -> dict:
    """
    í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    
    ìƒì„±ë˜ëŠ” í”¼ì²˜:
    1. rooms_per_household: ê°€êµ¬ë‹¹ ë°© ìˆ˜
    2. bedrooms_per_room: ë°©ë‹¹ ì¹¨ì‹¤ ë¹„ìœ¨
    3. population_per_household: ê°€êµ¬ë‹¹ ì¸êµ¬
    4. income_category: ì†Œë“ êµ¬ê°„ (ë²”ì£¼í˜•)
    5. location_cluster: ìœ„ì¹˜ í´ëŸ¬ìŠ¤í„° (í•´ì•ˆ ê·¼ì ‘ë„)
    """
    import pandas as pd
    import numpy as np
    
    print("=" * 60)
    print("Step 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
    print("=" * 60)
    
    df = pd.read_csv(input_data.path)
    original_features = list(df.columns)
    
    # ===== íŒŒìƒ ë³€ìˆ˜ 1: rooms_per_household =====
    df['rooms_per_household'] = df['AveRooms'] * df['AveOccup']
    print("âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„±: rooms_per_household (ê°€êµ¬ë‹¹ ì´ ë°© ìˆ˜)")
    
    # ===== íŒŒìƒ ë³€ìˆ˜ 2: bedrooms_per_room =====
    df['bedrooms_per_room'] = df['AveBedrms'] / df['AveRooms']
    # NaN ì²˜ë¦¬
    df['bedrooms_per_room'] = df['bedrooms_per_room'].fillna(0)
    print("âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„±: bedrooms_per_room (ë°©ë‹¹ ì¹¨ì‹¤ ë¹„ìœ¨)")
    
    # ===== íŒŒìƒ ë³€ìˆ˜ 3: population_per_household =====
    df['population_per_household'] = df['Population'] / df['AveOccup']
    df['population_per_household'] = df['population_per_household'].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)
    print("âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„±: population_per_household (ê°€êµ¬ë‹¹ ì¸êµ¬)")
    
    # ===== íŒŒìƒ ë³€ìˆ˜ 4: income_category =====
    df['income_category'] = pd.cut(
        df['MedInc'],
        bins=[0, 2, 4, 6, 8, np.inf],
        labels=[1, 2, 3, 4, 5]
    ).astype(int)
    print("âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„±: income_category (ì†Œë“ êµ¬ê°„ 1-5)")
    
    # ===== íŒŒìƒ ë³€ìˆ˜ 5: location_cluster =====
    # ìœ„ë„/ê²½ë„ ê¸°ë°˜ í•´ì•ˆ ê·¼ì ‘ë„ (ë‹¨ìˆœí™”ëœ ë²„ì „)
    # LA(34.05, -118.24), SF(37.77, -122.42) ê¸°ì¤€
    la_lat, la_lon = 34.05, -118.24
    sf_lat, sf_lon = 37.77, -122.42
    
    df['dist_to_la'] = np.sqrt(
        (df['Latitude'] - la_lat)**2 + (df['Longitude'] - la_lon)**2
    )
    df['dist_to_sf'] = np.sqrt(
        (df['Latitude'] - sf_lat)**2 + (df['Longitude'] - sf_lon)**2
    )
    df['location_cluster'] = (df['dist_to_la'] < df['dist_to_sf']).astype(int)
    # ì„ì‹œ ì»¬ëŸ¼ ì‚­ì œ
    df = df.drop(columns=['dist_to_la', 'dist_to_sf'])
    print("âœ… íŒŒìƒ ë³€ìˆ˜ ìƒì„±: location_cluster (0=SFê·¼ì ‘, 1=LAê·¼ì ‘)")
    
    # ê²°ê³¼ ìš”ì•½
    new_features = [col for col in df.columns if col not in original_features]
    print(f"\nğŸ“Š í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼:")
    print(f"   - ì›ë³¸ í”¼ì²˜: {len(original_features)}ê°œ")
    print(f"   - ìƒì„± í”¼ì²˜: {len(new_features)}ê°œ")
    print(f"   - ìµœì¢… í”¼ì²˜: {len(df.columns)}ê°œ")
    print(f"   - ìƒì„±ëœ í”¼ì²˜ ëª©ë¡: {new_features}")
    
    # ì €ì¥
    df.to_csv(output_data.path, index=False)
    print(f"\nâœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {output_data.path}")
    
    return {
        "original_features": len(original_features),
        "new_features": len(new_features),
        "total_features": len(df.columns),
        "new_feature_names": new_features
    }


# ============================================================
# Component 3: ë°ì´í„° ì „ì²˜ë¦¬
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["pandas==2.0.3", "scikit-learn==1.3.2", "joblib==1.3.2"]
)
def preprocess_data(
    input_data: Input[Dataset],
    X_train_out: Output[Dataset],
    X_test_out: Output[Dataset],
    y_train_out: Output[Dataset],
    y_test_out: Output[Dataset],
    scaler_out: Output[Model]
) -> dict:
    """ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§"""
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import joblib
    
    print("=" * 60)
    print("Step 3: ë°ì´í„° ì „ì²˜ë¦¬")
    print("=" * 60)
    
    df = pd.read_csv(input_data.path)
    
    # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
    X = df.drop(columns=['MedHouseVal'])
    y = df['MedHouseVal']
    
    print(f"í”¼ì²˜ shape: {X.shape}")
    print(f"íƒ€ê²Ÿ shape: {y.shape}")
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"\ní•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns
    )
    
    # ì €ì¥
    X_train_scaled.to_csv(X_train_out.path, index=False)
    X_test_scaled.to_csv(X_test_out.path, index=False)
    y_train.to_csv(y_train_out.path, index=False)
    y_test.to_csv(y_test_out.path, index=False)
    joblib.dump(scaler, scaler_out.path)
    
    print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
    
    return {
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "n_features": X_train.shape[1]
    }


# ============================================================
# Component 4: ëª¨ë¸ í•™ìŠµ + MLflow ì—°ë™ (20ì )
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=[
        "pandas==2.0.3", 
        "scikit-learn==1.3.2", 
        "mlflow==2.9.2",
        "boto3==1.34.0",
        "joblib==1.3.2"
    ]
)
def train_model_with_mlflow(
    X_train: Input[Dataset],
    y_train: Input[Dataset],
    X_test: Input[Dataset],
    y_test: Input[Dataset],
    mlflow_tracking_uri: str,
    experiment_name: str,
    team_name: str,
    model_out: Output[Model],
    metrics_out: Output[Metrics]
) -> NamedTuple('Outputs', [('run_id', str), ('r2_score', float), ('rmse', float)]):
    """
    ëª¨ë¸ í•™ìŠµ ë° MLflow ì—°ë™
    
    MLflow ì—°ë™ í•­ëª©:
    - Parameters: ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°
    - Metrics: R2, RMSE, MAE
    - Artifacts: ëª¨ë¸ íŒŒì¼
    - Tags: íŒ€ëª…, í™˜ê²½ ì •ë³´
    """
    import pandas as pd
    import numpy as np
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
    import mlflow
    import mlflow.sklearn
    import joblib
    from collections import namedtuple
    
    print("=" * 60)
    print("Step 4: ëª¨ë¸ í•™ìŠµ + MLflow ì—°ë™")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    X_train_df = pd.read_csv(X_train.path)
    y_train_df = pd.read_csv(y_train.path)
    X_test_df = pd.read_csv(X_test.path)
    y_test_df = pd.read_csv(y_test.path)
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)
    
    print(f"MLflow Tracking URI: {mlflow_tracking_uri}")
    print(f"Experiment: {experiment_name}")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
    params = {
        "n_estimators": 200,
        "max_depth": 15,
        "min_samples_split": 5,
        "min_samples_leaf": 2,
        "max_features": "sqrt",
        "random_state": 42,
        "n_jobs": -1
    }
    
    with mlflow.start_run(run_name=f"{team_name}-training") as run:
        run_id = run.info.run_id
        print(f"\nğŸ“ MLflow Run ID: {run_id}")
        
        # ===== Parameters ë¡œê¹… =====
        mlflow.log_params(params)
        print("âœ… Parameters ë¡œê¹… ì™„ë£Œ")
        
        # ===== Tags ì„¤ì • =====
        mlflow.set_tag("team", team_name)
        mlflow.set_tag("model_type", "RandomForestRegressor")
        mlflow.set_tag("dataset", "california_housing")
        mlflow.set_tag("feature_engineering", "enabled")
        print("âœ… Tags ì„¤ì • ì™„ë£Œ")
        
        # ===== ëª¨ë¸ í•™ìŠµ =====
        print("\nğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        model = RandomForestRegressor(**params)
        model.fit(X_train_df, y_train_df.values.ravel())
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # ===== ì˜ˆì¸¡ ë° í‰ê°€ =====
        y_pred = model.predict(X_test_df)
        
        r2 = r2_score(y_test_df, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test_df, y_pred))
        mae = mean_absolute_error(y_test_df, y_pred)
        
        # ===== Metrics ë¡œê¹… =====
        mlflow.log_metric("r2_score", r2)
        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)
        mlflow.log_metric("n_features", X_train_df.shape[1])
        mlflow.log_metric("n_train_samples", len(X_train_df))
        print("âœ… Metrics ë¡œê¹… ì™„ë£Œ")
        
        print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   - RÂ² Score: {r2:.4f}")
        print(f"   - RMSE: {rmse:.4f}")
        print(f"   - MAE: {mae:.4f}")
        
        # ===== Feature Importance ë¡œê¹… =====
        feature_importance = pd.DataFrame({
            'feature': X_train_df.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” Top 5 Feature Importance:")
        for idx, row in feature_importance.head(5).iterrows():
            print(f"   - {row['feature']}: {row['importance']:.4f}")
        
        # Feature Importanceë¥¼ artifactë¡œ ì €ì¥
        fi_path = "/tmp/feature_importance.csv"
        feature_importance.to_csv(fi_path, index=False)
        mlflow.log_artifact(fi_path)
        
        # ===== ëª¨ë¸ ì €ì¥ ë° ë¡œê¹… =====
        mlflow.sklearn.log_model(
            model, 
            "model",
            registered_model_name=f"{team_name}-housing-model"
        )
        print("âœ… ëª¨ë¸ MLflowì— ë“±ë¡ ì™„ë£Œ")
        
        # ë¡œì»¬ ì €ì¥
        joblib.dump(model, model_out.path)
    
    # KFP Metrics ì €ì¥
    metrics_out.log_metric("r2_score", r2)
    metrics_out.log_metric("rmse", rmse)
    metrics_out.log_metric("mae", mae)
    
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! Run ID: {run_id}")
    
    Outputs = namedtuple('Outputs', ['run_id', 'r2_score', 'rmse'])
    return Outputs(run_id, r2, rmse)


# ============================================================
# Component 5: ëª¨ë¸ í‰ê°€ ë° ë°°í¬ ê²°ì •
# ============================================================
@component(base_image="python:3.9-slim")
def evaluate_and_decide(
    r2_score: float,
    rmse: float,
    r2_threshold: float = 0.75,
    rmse_threshold: float = 0.6
) -> bool:
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë°°í¬ ì—¬ë¶€ ê²°ì •"""
    
    print("=" * 60)
    print("Step 5: ë°°í¬ ê²°ì •")
    print("=" * 60)
    
    print(f"í˜„ì¬ ì„±ëŠ¥:")
    print(f"  - RÂ² Score: {r2_score:.4f} (ì„ê³„ê°’: {r2_threshold})")
    print(f"  - RMSE: {rmse:.4f} (ì„ê³„ê°’: {rmse_threshold})")
    
    deploy = r2_score >= r2_threshold and rmse <= rmse_threshold
    
    if deploy:
        print(f"\nâœ… ë°°í¬ ìŠ¹ì¸! ëª¨ë¸ì´ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•©ë‹ˆë‹¤.")
    else:
        print(f"\nâŒ ë°°í¬ ê±°ë¶€. ëª¨ë¸ ì„±ëŠ¥ì´ ê¸°ì¤€ ë¯¸ë‹¬ì…ë‹ˆë‹¤.")
        if r2_score < r2_threshold:
            print(f"   - RÂ² Scoreê°€ {r2_threshold} ë¯¸ë§Œì…ë‹ˆë‹¤.")
        if rmse > rmse_threshold:
            print(f"   - RMSEê°€ {rmse_threshold} ì´ˆê³¼ì…ë‹ˆë‹¤.")
    
    return deploy


# ============================================================
# Component 6: KServe ë°°í¬ (25ì )
# ============================================================
@component(
    base_image="python:3.9-slim",
    packages_to_install=["kubernetes==28.1.0", "boto3==1.34.0"]
)
def deploy_to_kserve(
    model: Input[Model],
    team_name: str,
    namespace: str,
    s3_bucket: str,
    run_id: str
) -> str:
    """KServe InferenceService ë°°í¬"""
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import boto3
    import os
    import shutil
    
    print("=" * 60)
    print("Step 6: KServe ë°°í¬")
    print("=" * 60)
    
    # ===== S3 ì—…ë¡œë“œ =====
    print("\nğŸ“¤ ëª¨ë¸ì„ S3ì— ì—…ë¡œë“œ ì¤‘...")
    
    s3_client = boto3.client('s3')
    s3_path = f"models/{team_name}/{run_id}/model.joblib"
    
    s3_client.upload_file(model.path, s3_bucket, s3_path)
    model_uri = f"s3://{s3_bucket}/models/{team_name}/{run_id}/"
    
    print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {model_uri}")
    
    # ===== InferenceService ìƒì„± =====
    print("\nğŸš€ InferenceService ìƒì„± ì¤‘...")
    
    try:
        config.load_incluster_config()
    except:
        config.load_kube_config()
    
    api = client.CustomObjectsApi()
    
    service_name = f"{team_name}-housing-predictor"
    
    inference_service = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": service_name,
            "namespace": namespace,
            "labels": {
                "team": team_name,
                "project": "housing-prediction",
                "mlflow-run-id": run_id
            },
            "annotations": {
                "sidecar.istio.io/inject": "true"
            }
        },
        "spec": {
            "predictor": {
                "sklearn": {
                    "storageUri": model_uri,
                    "resources": {
                        "requests": {
                            "cpu": "100m",
                            "memory": "256Mi"
                        },
                        "limits": {
                            "cpu": "500m",
                            "memory": "512Mi"
                        }
                    }
                }
            }
        }
    }
    
    # ê¸°ì¡´ ì„œë¹„ìŠ¤ ì‚­ì œ (ìˆëŠ” ê²½ìš°)
    try:
        api.delete_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            name=service_name
        )
        print(f"âš ï¸ ê¸°ì¡´ InferenceService '{service_name}' ì‚­ì œë¨")
        import time
        time.sleep(5)  # ì‚­ì œ ì™„ë£Œ ëŒ€ê¸°
    except ApiException as e:
        if e.status != 404:
            raise
    
    # ìƒˆ ì„œë¹„ìŠ¤ ìƒì„±
    result = api.create_namespaced_custom_object(
        group="serving.kserve.io",
        version="v1beta1",
        namespace=namespace,
        plural="inferenceservices",
        body=inference_service
    )
    
    print(f"âœ… InferenceService ìƒì„± ì™„ë£Œ!")
    print(f"   - ì´ë¦„: {service_name}")
    print(f"   - ë„¤ì„ìŠ¤í˜ì´ìŠ¤: {namespace}")
    print(f"   - ëª¨ë¸ URI: {model_uri}")
    
    # ì—”ë“œí¬ì¸íŠ¸ URL ë°˜í™˜
    endpoint_url = f"http://{service_name}.{namespace}.svc.cluster.local/v1/models/{service_name}:predict"
    
    print(f"\nğŸ“¡ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸:")
    print(f"   {endpoint_url}")
    
    print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:")
    print(f'''
curl -X POST {endpoint_url} \\
  -H "Content-Type: application/json" \\
  -d '{{"instances": [[8.3252, 41.0, 6.984, 1.0238, 322.0, 2.555, 37.88, -122.23, 17.87, 0.146, 126.0, 3, 0]]}}'
''')
    
    return endpoint_url


# ============================================================
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì •ì˜
# ============================================================
@dsl.pipeline(
    name="california-housing-mlops-pipeline",
    description="California Housing E2E MLOps Pipeline - í”„ë¡œì íŠ¸ ì†”ë£¨ì…˜"
)
def housing_mlops_pipeline(
    team_name: str = "team-01",
    namespace: str = "kubeflow-user01",
    mlflow_tracking_uri: str = "http://mlflow-server.mlflow-system.svc.cluster.local:5000",
    experiment_name: str = "california-housing-project",
    s3_bucket: str = "mlops-training-bucket",
    r2_threshold: float = 0.75,
    rmse_threshold: float = 0.6
):
    """
    E2E MLOps íŒŒì´í”„ë¼ì¸
    
    íŒŒì´í”„ë¼ì¸ íë¦„:
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ load_data  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ feature_engineering â”‚ â† íŒŒìƒ ë³€ìˆ˜ 5ê°œ ìƒì„±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ preprocess_data â”‚ â† ë¶„í•  + ìŠ¤ì¼€ì¼ë§
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ train_model_with_mlflow â”‚ â† MLflow ì—°ë™
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ evaluate_and_decide â”‚ â† ë°°í¬ ê²°ì •
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼ (ì¡°ê±´ë¶€)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ deploy_to_kserve â”‚ â† KServe ë°°í¬
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    # Step 1: ë°ì´í„° ë¡œë“œ
    load_task = load_data()
    
    # Step 2: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    fe_task = feature_engineering(
        input_data=load_task.outputs["output_data"]
    )
    
    # Step 3: ì „ì²˜ë¦¬
    preprocess_task = preprocess_data(
        input_data=fe_task.outputs["output_data"]
    )
    
    # Step 4: ëª¨ë¸ í•™ìŠµ + MLflow
    train_task = train_model_with_mlflow(
        X_train=preprocess_task.outputs["X_train_out"],
        y_train=preprocess_task.outputs["y_train_out"],
        X_test=preprocess_task.outputs["X_test_out"],
        y_test=preprocess_task.outputs["y_test_out"],
        mlflow_tracking_uri=mlflow_tracking_uri,
        experiment_name=experiment_name,
        team_name=team_name
    )
    
    # Step 5: í‰ê°€ ë° ë°°í¬ ê²°ì •
    eval_task = evaluate_and_decide(
        r2_score=train_task.outputs["r2_score"],
        rmse=train_task.outputs["rmse"],
        r2_threshold=r2_threshold,
        rmse_threshold=rmse_threshold
    )
    
    # Step 6: KServe ë°°í¬ (ì¡°ê±´ë¶€)
    with dsl.Condition(eval_task.output == True, name="deploy-if-approved"):
        deploy_task = deploy_to_kserve(
            model=train_task.outputs["model_out"],
            team_name=team_name,
            namespace=namespace,
            s3_bucket=s3_bucket,
            run_id=train_task.outputs["run_id"]
        )


# ============================================================
# ì»´íŒŒì¼ ë° ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    from kfp import compiler
    import os
    
    # ì»´íŒŒì¼
    output_file = "california_housing_pipeline.yaml"
    compiler.Compiler().compile(
        pipeline_func=housing_mlops_pipeline,
        package_path=output_file
    )
    
    print("=" * 60)
    print("âœ… íŒŒì´í”„ë¼ì¸ ì»´íŒŒì¼ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“„ ì¶œë ¥ íŒŒì¼: {output_file}")
    print(f"\nğŸš€ ì‹¤í–‰ ë°©ë²•:")
    print(f"   1. Kubeflow UI â†’ Pipelines â†’ Upload Pipeline")
    print(f"   2. '{output_file}' ì—…ë¡œë“œ")
    print(f"   3. Create Run â†’ íŒŒë¼ë¯¸í„° ì…ë ¥:")
    print(f"      - team_name: íŒ€ ì´ë¦„ (ì˜ˆ: team-01)")
    print(f"      - namespace: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ì˜ˆ: kubeflow-user01)")
    print(f"      - s3_bucket: S3 ë²„í‚·ëª…")
    print(f"   4. Start í´ë¦­")
    print("=" * 60)
