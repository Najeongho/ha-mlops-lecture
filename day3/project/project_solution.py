"""
Day 3 ÌîÑÎ°úÏ†ùÌä∏: MLOps ÌååÏù¥ÌîÑÎùºÏù∏ - ÏÜîÎ£®ÏÖò
==========================================

ÌòÑÎåÄÏò§ÌÜ†ÏóêÎ≤Ñ MLOps ÍµêÏú°
KFP v1 API (kfp==1.8.22)

Ïù¥ ÌååÏùºÏùÄ ÌîÑÎ°úÏ†ùÌä∏ ÌÖúÌîåÎ¶øÏùò ÏôÑÏÑ±Îêú ÏÜîÎ£®ÏÖòÏûÖÎãàÎã§.
ÏàòÍ∞ïÏÉùÎì§Ïù¥ ÏßÅÏ†ë Íµ¨ÌòÑÌïú ÌõÑ Ï∞∏Í≥†Ïö©ÏúºÎ°ú ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
"""

from kfp import dsl
from kfp.components import create_component_from_func
from kfp import compiler


# ============================================================
# Component 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
# ============================================================

@create_component_from_func
def load_data(dataset_name: str = "california") -> str:
    """
    Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† Ï†ÄÏû•Ìï©ÎãàÎã§.
    
    Args:
        dataset_name: Îç∞Ïù¥ÌÑ∞ÏÖã Ïù¥Î¶Ñ
    
    Returns:
        Ï†ÄÏû•Îêú ÌååÏùº Í≤ΩÎ°ú
    """
    import pandas as pd
    from sklearn.datasets import fetch_california_housing
    
    print("=" * 50)
    print("  Step 1: Load Data")
    print("=" * 50)
    
    if dataset_name == "california":
        data = fetch_california_housing()
        df = pd.DataFrame(data.data, columns=data.feature_names)
        df['target'] = data.target
    else:
        raise ValueError(f"Unknown dataset: {dataset_name}")
    
    output_path = "/tmp/raw_data.csv"
    df.to_csv(output_path, index=False)
    
    print(f"  ‚úÖ Data loaded: {len(df)} rows, {len(df.columns)} columns")
    print(f"  ‚úÖ Features: {list(df.columns[:-1])}")
    print(f"  ‚úÖ Saved to: {output_path}")
    
    return output_path


# ============================================================
# Component 2: Ï†ÑÏ≤òÎ¶¨
# ============================================================

@create_component_from_func
def preprocess(data_path: str, test_size: float = 0.2) -> str:
    """
    Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î∞è Train/Test Î∂ÑÌï†
    
    Args:
        data_path: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
        test_size: ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏ ÎπÑÏú®
    
    Returns:
        Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
    """
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import json
    import os
    
    print("=" * 50)
    print("  Step 2: Preprocess")
    print("=" * 50)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    df = pd.read_csv(data_path)
    print(f"  Loaded: {len(df)} rows")
    
    # ÌîºÏ≤ò/ÌÉÄÍ≤ü Î∂ÑÎ¶¨
    X = df.drop('target', axis=1)
    y = df['target']
    
    # Train/Test Î∂ÑÌï†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    print(f"  ‚úÖ Train: {len(X_train)}, Test: {len(X_test)}")
    
    # Ï†ïÍ∑úÌôî
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Ï†ÄÏû•
    output_dir = "/tmp/processed"
    os.makedirs(output_dir, exist_ok=True)
    
    np.save(f"{output_dir}/X_train.npy", X_train_scaled)
    np.save(f"{output_dir}/X_test.npy", X_test_scaled)
    np.save(f"{output_dir}/y_train.npy", y_train.values)
    np.save(f"{output_dir}/y_test.npy", y_test.values)
    
    # Ïä§ÏºÄÏùºÎü¨ Ï†ïÎ≥¥ Ï†ÄÏû•
    metadata = {
        "n_train": len(X_train),
        "n_test": len(X_test),
        "n_features": X_train.shape[1],
        "feature_names": list(X.columns),
        "scaler_mean": scaler.mean_.tolist(),
        "scaler_scale": scaler.scale_.tolist()
    }
    
    with open(f"{output_dir}/metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"  ‚úÖ Saved to: {output_dir}")
    
    return output_dir


# ============================================================
# Component 3: ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ
# ============================================================

@create_component_from_func
def hyperparameter_search(
    data_dir: str,
    mlflow_tracking_uri: str,
    experiment_name: str
) -> str:
    """
    ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ ÏàòÌñâ
    
    Args:
        data_dir: Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
        experiment_name: Ïã§Ìóò Ïù¥Î¶Ñ
    
    Returns:
        ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ (JSON)
    """
    import numpy as np
    import mlflow
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import r2_score
    import json
    import os
    
    print("=" * 50)
    print("  Step 3: Hyperparameter Search")
    print("=" * 50)
    
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name + "-hp-search")
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    X_train = np.load(f"{data_dir}/X_train.npy")
    X_test = np.load(f"{data_dir}/X_test.npy")
    y_train = np.load(f"{data_dir}/y_train.npy")
    y_test = np.load(f"{data_dir}/y_test.npy")
    
    # ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Í∑∏Î¶¨Îìú
    param_grid = [
        {"n_estimators": 50, "max_depth": 5},
        {"n_estimators": 100, "max_depth": 10},
        {"n_estimators": 150, "max_depth": 15},
        {"n_estimators": 200, "max_depth": 10},
    ]
    
    best_score = 0
    best_params = None
    
    print(f"\n  Testing {len(param_grid)} configurations...")
    
    for params in param_grid:
        with mlflow.start_run(nested=True):
            mlflow.log_params(params)
            
            model = RandomForestRegressor(
                n_estimators=params["n_estimators"],
                max_depth=params["max_depth"],
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_train, y_train)
            
            y_pred = model.predict(X_test)
            r2 = r2_score(y_test, y_pred)
            
            mlflow.log_metric("r2", r2)
            
            print(f"    n_estimators={params['n_estimators']}, "
                  f"max_depth={params['max_depth']}: R2={r2:.4f}")
            
            if r2 > best_score:
                best_score = r2
                best_params = params.copy()
    
    print(f"\n  üèÜ Best: n_estimators={best_params['n_estimators']}, "
          f"max_depth={best_params['max_depth']}, R2={best_score:.4f}")
    
    return json.dumps(best_params)


# ============================================================
# Component 4: Î™®Îç∏ ÌïôÏäµ
# ============================================================

@create_component_from_func
def train_model(
    data_dir: str,
    best_params: str,
    mlflow_tracking_uri: str,
    experiment_name: str
) -> str:
    """
    ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î°ú Î™®Îç∏ ÌïôÏäµ
    
    Args:
        data_dir: Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨
        best_params: ÏµúÏ†Å ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ (JSON)
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
        experiment_name: Ïã§Ìóò Ïù¥Î¶Ñ
    
    Returns:
        MLflow Run ID
    """
    import numpy as np
    import mlflow
    import mlflow.sklearn
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    import json
    import os
    
    print("=" * 50)
    print("  Step 4: Train Model")
    print("=" * 50)
    
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)
    
    # ÌååÎùºÎØ∏ÌÑ∞ ÌååÏã±
    params = json.loads(best_params)
    print(f"  Using params: {params}")
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    X_train = np.load(f"{data_dir}/X_train.npy")
    X_test = np.load(f"{data_dir}/X_test.npy")
    y_train = np.load(f"{data_dir}/y_train.npy")
    y_test = np.load(f"{data_dir}/y_test.npy")
    
    with mlflow.start_run() as run:
        run_id = run.info.run_id
        
        # ÌååÎùºÎØ∏ÌÑ∞ Î°úÍπÖ
        mlflow.log_params({
            "n_estimators": params["n_estimators"],
            "max_depth": params["max_depth"],
            "random_state": 42,
            "n_jobs": -1
        })
        
        # Î™®Îç∏ ÌïôÏäµ
        print("\n  Training model...")
        model = RandomForestRegressor(
            n_estimators=params["n_estimators"],
            max_depth=params["max_depth"],
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        
        # ÌèâÍ∞Ä
        y_pred = model.predict(X_test)
        
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # Î©îÌä∏Î¶≠ Î°úÍπÖ
        mlflow.log_metrics({
            "mse": mse,
            "rmse": rmse,
            "mae": mae,
            "r2": r2
        })
        
        # Î™®Îç∏ Ï†ÄÏû•
        mlflow.sklearn.log_model(
            model, "model",
            registered_model_name="project-california-model"
        )
        
        # ÌÉúÍ∑∏
        mlflow.set_tag("pipeline", "project")
        mlflow.set_tag("stage", "production")
        
        print(f"\n  ‚úÖ Model trained!")
        print(f"     R2: {r2:.4f}")
        print(f"     RMSE: {rmse:.4f}")
        print(f"     Run ID: {run_id}")
    
    return run_id


# ============================================================
# Component 5: Î™®Îç∏ ÌèâÍ∞Ä
# ============================================================

@create_component_from_func
def evaluate_model(
    run_id: str,
    mlflow_tracking_uri: str,
    r2_threshold: float = 0.8
) -> str:
    """
    Î™®Îç∏ ÌèâÍ∞Ä Î∞è Î∞∞Ìè¨ Í≤∞Ï†ï
    
    Args:
        run_id: MLflow Run ID
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
        r2_threshold: R2 ÏûÑÍ≥ÑÍ∞í
    
    Returns:
        "deploy" ÎòêÎäî "skip"
    """
    import mlflow
    import os
    
    print("=" * 50)
    print("  Step 5: Evaluate Model")
    print("=" * 50)
    
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    # Run Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
    client = mlflow.tracking.MlflowClient()
    run = client.get_run(run_id)
    
    r2 = float(run.data.metrics.get("r2", 0))
    rmse = float(run.data.metrics.get("rmse", 0))
    
    print(f"  Run ID: {run_id}")
    print(f"  R2 Score: {r2:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  Threshold: {r2_threshold}")
    
    if r2 >= r2_threshold:
        decision = "deploy"
        print(f"\n  ‚úÖ Decision: DEPLOY")
        print(f"     R2 ({r2:.4f}) >= Threshold ({r2_threshold})")
    else:
        decision = "skip"
        print(f"\n  ‚ö†Ô∏è Decision: SKIP")
        print(f"     R2 ({r2:.4f}) < Threshold ({r2_threshold})")
    
    # Í≤∞Ï†ï Í∏∞Î°ù
    with mlflow.start_run(run_id=run_id):
        mlflow.set_tag("deployment_decision", decision)
    
    return decision


# ============================================================
# Component 6: Î™®Îç∏ Î∞∞Ìè¨
# ============================================================

@create_component_from_func
def deploy_model(
    run_id: str,
    model_name: str,
    namespace: str,
    mlflow_tracking_uri: str
):
    """
    KServe InferenceServiceÎ°ú Î™®Îç∏ Î∞∞Ìè¨
    
    Args:
        run_id: MLflow Run ID
        model_name: Î™®Îç∏ Ïù¥Î¶Ñ
        namespace: Kubernetes ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
    """
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import time
    
    print("=" * 50)
    print("  Step 6: Deploy Model")
    print("=" * 50)
    
    print(f"  Model: {model_name}")
    print(f"  Namespace: {namespace}")
    print(f"  Run ID: {run_id}")
    
    # Kubernetes ÏÑ§Ï†ï
    try:
        config.load_incluster_config()
        print("  ‚úÖ In-cluster config loaded")
    except:
        config.load_kube_config()
        print("  ‚úÖ Kubeconfig loaded")
    
    model_uri = f"s3://mlflow-artifacts/{run_id}/artifacts/model"
    
    isvc = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": model_name,
            "namespace": namespace,
            "labels": {
                "app": model_name,
                "mlflow-run-id": run_id
            },
            "annotations": {
                "autoscaling.knative.dev/minScale": "1"
            }
        },
        "spec": {
            "predictor": {
                "sklearn": {
                    "storageUri": model_uri,
                    "resources": {
                        "requests": {"cpu": "100m", "memory": "256Mi"},
                        "limits": {"cpu": "500m", "memory": "512Mi"}
                    }
                }
            }
        }
    }
    
    api = client.CustomObjectsApi()
    
    # Í∏∞Ï°¥ Î¶¨ÏÜåÏä§ ÏÇ≠Ï†ú
    try:
        api.delete_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            name=model_name
        )
        print(f"  ‚ö†Ô∏è Deleted existing InferenceService")
        time.sleep(5)
    except ApiException as e:
        if e.status != 404:
            raise
    
    # ÏÉùÏÑ±
    api.create_namespaced_custom_object(
        group="serving.kserve.io",
        version="v1beta1",
        namespace=namespace,
        plural="inferenceservices",
        body=isvc
    )
    
    print(f"\n  ‚úÖ InferenceService created: {model_name}")
    print(f"  Endpoint: http://{model_name}.{namespace}.svc.cluster.local")


# ============================================================
# Component 7: ÏïåÎ¶º
# ============================================================

@create_component_from_func
def send_alert(run_id: str, message: str = "Model did not meet threshold"):
    """ÏÑ±Îä• ÎØ∏Îã¨ ÏïåÎ¶º"""
    print("=" * 50)
    print("  Step 6 (Alt): Send Alert")
    print("=" * 50)
    
    print(f"  ‚ö†Ô∏è ALERT: {message}")
    print(f"  Run ID: {run_id}")
    print(f"\n  Action Required:")
    print(f"    1. Check MLflow for detailed metrics")
    print(f"    2. Review training data quality")
    print(f"    3. Consider hyperparameter tuning")


# ============================================================
# Pipeline Definition
# ============================================================

@dsl.pipeline(
    name='project-mlops-pipeline',
    description='Complete MLOps Pipeline with HP Search and Conditional Deployment'
)
def project_pipeline(
    dataset_name: str = "california",
    mlflow_tracking_uri: str = "http://mlflow-server-service.mlflow-system.svc.cluster.local:5000",
    experiment_name: str = "project-experiment",
    model_name: str = "project-model",
    namespace: str = "kubeflow-user01",
    r2_threshold: float = 0.8
):
    """
    ÏôÑÏ†ÑÌïú MLOps ÌååÏù¥ÌîÑÎùºÏù∏
    
    Flow:
    load_data ‚Üí preprocess ‚Üí hp_search ‚Üí train ‚Üí evaluate ‚Üí deploy/alert
    """
    
    # Step 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    load_task = load_data(dataset_name=dataset_name)
    
    # Step 2: Ï†ÑÏ≤òÎ¶¨
    preprocess_task = preprocess(data_path=load_task.output)
    
    # Step 3: ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ
    hp_task = hyperparameter_search(
        data_dir=preprocess_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri,
        experiment_name=experiment_name
    )
    
    # Step 4: Î™®Îç∏ ÌïôÏäµ
    train_task = train_model(
        data_dir=preprocess_task.output,
        best_params=hp_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri,
        experiment_name=experiment_name
    )
    
    # Step 5: ÌèâÍ∞Ä
    evaluate_task = evaluate_model(
        run_id=train_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri,
        r2_threshold=r2_threshold
    )
    
    # Step 6: Ï°∞Í±¥Î∂Ä Î∞∞Ìè¨
    with dsl.Condition(evaluate_task.output == "deploy", name="deploy-condition"):
        deploy_model(
            run_id=train_task.output,
            model_name=model_name,
            namespace=namespace,
            mlflow_tracking_uri=mlflow_tracking_uri
        )
    
    with dsl.Condition(evaluate_task.output == "skip", name="skip-condition"):
        send_alert(run_id=train_task.output)


# ============================================================
# Main - Compile Pipeline
# ============================================================

if __name__ == "__main__":
    import os
    
    print("=" * 60)
    print("  Project Pipeline - Solution")
    print("=" * 60)
    
    # Ïª¥ÌååÏùº
    output_file = "project_pipeline_solution.yaml"
    
    compiler.Compiler().compile(
        pipeline_func=project_pipeline,
        package_path=output_file
    )
    
    print(f"\n‚úÖ Pipeline compiled: {output_file}")
    print("\nüìã Next Steps:")
    print("  1. Upload to Kubeflow UI")
    print("  2. Create a Run with your parameters")
    print("  3. Monitor in Kubeflow Dashboard")
    print("  4. Check MLflow for experiments")
