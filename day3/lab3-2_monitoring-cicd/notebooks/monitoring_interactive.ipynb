{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3-2: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ëŒ€í™”í˜• ì‹¤ìŠµ\n",
    "\n",
    "Jupyter Notebookì„ í†µí•´ Prometheus ë©”íŠ¸ë¦­ê³¼ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ ëŒ€í™”í˜•ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ“‹ ì‹¤ìŠµ ë‚´ìš©\n",
    "\n",
    "1. Prometheus ë©”íŠ¸ë¦­ ìƒì„± ë° ìˆ˜ì§‘\n",
    "2. Custom Metrics Exporter êµ¬í˜„\n",
    "3. PromQL ì¿¼ë¦¬ ì‹¤ìŠµ\n",
    "4. ë©”íŠ¸ë¦­ ì‹œê°í™”\n",
    "5. A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜\n",
    "6. ì•Œë¦¼ ê·œì¹™ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  importí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install prometheus-client requests pandas numpy matplotlib seaborn -q\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from prometheus_client import start_http_server, Gauge, Counter, Histogram, Summary\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prometheus ë©”íŠ¸ë¦­ íƒ€ì… ì´í•´\n",
    "\n",
    "PrometheusëŠ” 4ê°€ì§€ ì£¼ìš” ë©”íŠ¸ë¦­ íƒ€ì…ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Counter (ì¹´ìš´í„°)\n",
    "\n",
    "- ë‹¨ì¡° ì¦ê°€í•˜ëŠ” ê°’ (0ì—ì„œ ì‹œì‘, ì¦ê°€ë§Œ ê°€ëŠ¥)\n",
    "- ì˜ˆ: ìš”ì²­ ìˆ˜, ì—ëŸ¬ ìˆ˜, ì™„ë£Œëœ ì‘ì—… ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter ìƒì„±\n",
    "prediction_counter = Counter(\n",
    "    'model_predictions_total',\n",
    "    'Total number of predictions',\n",
    "    ['model_version', 'status']\n",
    ")\n",
    "\n",
    "# Counter ì¦ê°€\n",
    "print(\"=\" * 60)\n",
    "print(\"Counter ì‹¤ìŠµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(10):\n",
    "    # ì„±ê³µ ì˜ˆì¸¡\n",
    "    if random.random() > 0.1:  # 90% ì„±ê³µ\n",
    "        prediction_counter.labels(model_version='v1.0', status='success').inc()\n",
    "    else:  # 10% ì‹¤íŒ¨\n",
    "        prediction_counter.labels(model_version='v1.0', status='error').inc()\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\nâœ… Counter ì¦ê°€ ì™„ë£Œ\")\n",
    "print(f\"í˜„ì¬ ê°’ì€ /metrics ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gauge (ê²Œì´ì§€)\n",
    "\n",
    "- ì¦ê°€/ê°ì†Œ ëª¨ë‘ ê°€ëŠ¥í•œ ê°’\n",
    "- ì˜ˆ: CPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, í˜„ì¬ í™œì„± ì‚¬ìš©ì ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gauge ìƒì„±\n",
    "mae_gauge = Gauge(\n",
    "    'model_mae_score',\n",
    "    'Current MAE score of the model',\n",
    "    ['model_version']\n",
    ")\n",
    "\n",
    "r2_gauge = Gauge(\n",
    "    'model_r2_score',\n",
    "    'Current RÂ² score of the model',\n",
    "    ['model_version']\n",
    ")\n",
    "\n",
    "# Gauge ê°’ ì‹œë®¬ë ˆì´ì…˜\n",
    "print(\"=\" * 60)\n",
    "print(\"Gauge ì‹¤ìŠµ - ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mae_values = []\n",
    "r2_values = []\n",
    "\n",
    "for i in range(20):\n",
    "    # v1.0 ëª¨ë¸ - ì•ˆì •ì \n",
    "    mae_v1 = 0.445 + random.gauss(0, 0.01)\n",
    "    r2_v1 = 0.798 + random.gauss(0, 0.005)\n",
    "    \n",
    "    # v2.0 ëª¨ë¸ - ë” ì¢‹ì€ ì„±ëŠ¥\n",
    "    mae_v2 = 0.362 + random.gauss(0, 0.01)\n",
    "    r2_v2 = 0.885 + random.gauss(0, 0.005)\n",
    "    \n",
    "    # Gauge ì—…ë°ì´íŠ¸\n",
    "    mae_gauge.labels(model_version='v1.0').set(mae_v1)\n",
    "    r2_gauge.labels(model_version='v1.0').set(r2_v1)\n",
    "    mae_gauge.labels(model_version='v2.0').set(mae_v2)\n",
    "    r2_gauge.labels(model_version='v2.0').set(r2_v2)\n",
    "    \n",
    "    mae_values.append({'v1.0': mae_v1, 'v2.0': mae_v2})\n",
    "    r2_values.append({'v1.0': r2_v1, 'v2.0': r2_v2})\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Step {i+1:2d} | v1.0: MAE={mae_v1:.4f}, RÂ²={r2_v1:.4f} | v2.0: MAE={mae_v2:.4f}, RÂ²={r2_v2:.4f}\")\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "print(\"\\nâœ… Gauge ì—…ë°ì´íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Histogram (íˆìŠ¤í† ê·¸ë¨)\n",
    "\n",
    "- ê°’ì˜ ë¶„í¬ë¥¼ ì¸¡ì • (ë²„í‚·ë³„ ë¹ˆë„)\n",
    "- ì˜ˆ: ì‘ë‹µ ì‹œê°„, ìš”ì²­ í¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram ìƒì„±\n",
    "latency_histogram = Histogram(\n",
    "    'model_prediction_latency_seconds',\n",
    "    'Prediction latency in seconds',\n",
    "    ['model_version'],\n",
    "    buckets=(0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)\n",
    ")\n",
    "\n",
    "# Latency ì‹œë®¬ë ˆì´ì…˜\n",
    "print(\"=\" * 60)\n",
    "print(\"Histogram ì‹¤ìŠµ - ì˜ˆì¸¡ ë ˆì´í„´ì‹œ ì¸¡ì •\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "latencies_v1 = []\n",
    "latencies_v2 = []\n",
    "\n",
    "for i in range(100):\n",
    "    # v1.0: í‰ê·  50ms, ë” í° ë³€ë™\n",
    "    latency_v1 = abs(random.gauss(0.05, 0.02))\n",
    "    latency_histogram.labels(model_version='v1.0').observe(latency_v1)\n",
    "    latencies_v1.append(latency_v1)\n",
    "    \n",
    "    # v2.0: í‰ê·  40ms, ë” ì•ˆì •ì \n",
    "    latency_v2 = abs(random.gauss(0.04, 0.01))\n",
    "    latency_histogram.labels(model_version='v2.0').observe(latency_v2)\n",
    "    latencies_v2.append(latency_v2)\n",
    "\n",
    "print(f\"\\nğŸ“Š v1.0 ë ˆì´í„´ì‹œ: í‰ê·  {np.mean(latencies_v1)*1000:.2f}ms, p95 {np.percentile(latencies_v1, 95)*1000:.2f}ms\")\n",
    "print(f\"ğŸ“Š v2.0 ë ˆì´í„´ì‹œ: í‰ê·  {np.mean(latencies_v2)*1000:.2f}ms, p95 {np.percentile(latencies_v2, 95)*1000:.2f}ms\")\n",
    "print(\"\\nâœ… Histogram ê´€ì¸¡ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Summary (ìš”ì•½)\n",
    "\n",
    "- Histogramê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë¶„ìœ„ìˆ˜ ê³„ì‚°\n",
    "- ì˜ˆ: ìš”ì²­ ì§€ì† ì‹œê°„, ì‘ë‹µ í¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary ìƒì„±\n",
    "request_size_summary = Summary(\n",
    "    'model_request_size_bytes',\n",
    "    'Size of prediction requests in bytes',\n",
    "    ['model_version']\n",
    ")\n",
    "\n",
    "# Request size ì‹œë®¬ë ˆì´ì…˜\n",
    "print(\"=\" * 60)\n",
    "print(\"Summary ì‹¤ìŠµ - ìš”ì²­ í¬ê¸° ì¸¡ì •\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(50):\n",
    "    # 100-500 bytes\n",
    "    size_v1 = random.randint(100, 500)\n",
    "    request_size_summary.labels(model_version='v1.0').observe(size_v1)\n",
    "    \n",
    "    size_v2 = random.randint(100, 500)\n",
    "    request_size_summary.labels(model_version='v2.0').observe(size_v2)\n",
    "\n",
    "print(\"\\nâœ… Summary ê´€ì¸¡ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Prometheus HTTP Server ì‹œì‘\n",
    "\n",
    "ë©”íŠ¸ë¦­ì„ `/metrics` ì—”ë“œí¬ì¸íŠ¸ë¡œ ë…¸ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus HTTP Server ì‹œì‘\n",
    "PORT = 8000\n",
    "\n",
    "try:\n",
    "    start_http_server(PORT)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… Prometheus Metrics Server ì‹œì‘!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nğŸ“¡ Metrics endpoint: http://localhost:{PORT}/metrics\")\n",
    "    print(\"\\nìƒˆ í„°ë¯¸ë„ì—ì„œ í™•ì¸:\")\n",
    "    print(f\"  curl http://localhost:{PORT}/metrics\")\n",
    "    print(\"\\në˜ëŠ” ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†í•˜ì„¸ìš”!\")\n",
    "except OSError:\n",
    "    print(f\"âš ï¸  í¬íŠ¸ {PORT}ì´(ê°€) ì´ë¯¸ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤\")\n",
    "    print(\"   Metrics serverê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ë©”íŠ¸ë¦­ ì‹œê°í™”\n",
    "\n",
    "ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE ì‹œê°í™”\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAE ë¹„êµ\n",
    "mae_df = pd.DataFrame(mae_values)\n",
    "mae_df.plot(ax=ax1, title='Model MAE Over Time', ylabel='MAE', xlabel='Step')\n",
    "ax1.legend(['v1.0', 'v2.0'])\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RÂ² ë¹„êµ\n",
    "r2_df = pd.DataFrame(r2_values)\n",
    "r2_df.plot(ax=ax2, title='Model RÂ² Score Over Time', ylabel='RÂ² Score', xlabel='Step')\n",
    "ax2.legend(['v1.0', 'v2.0'])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(f\"  v1.0: MAE={mae_df['v1.0'].mean():.4f}, RÂ²={r2_df['v1.0'].mean():.4f}\")\n",
    "print(f\"  v2.0: MAE={mae_df['v2.0'].mean():.4f}, RÂ²={r2_df['v2.0'].mean():.4f}\")\n",
    "print(f\"\\nâœ¨ v2.0ì´ MAE {(1 - mae_df['v2.0'].mean()/mae_df['v1.0'].mean())*100:.1f}% ê°œì„ !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency ë¶„í¬ ì‹œê°í™”\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(latencies_v1, bins=30, alpha=0.7, label='v1.0', color='blue')\n",
    "ax1.hist(latencies_v2, bins=30, alpha=0.7, label='v2.0', color='orange')\n",
    "ax1.set_xlabel('Latency (seconds)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Prediction Latency Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "latency_df = pd.DataFrame({'v1.0': latencies_v1, 'v2.0': latencies_v2})\n",
    "latency_df.boxplot(ax=ax2)\n",
    "ax2.set_ylabel('Latency (seconds)')\n",
    "ax2.set_title('Latency Comparison')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š ë ˆì´í„´ì‹œ í†µê³„:\")\n",
    "print(\"\\nv1.0:\")\n",
    "print(f\"  í‰ê· : {np.mean(latencies_v1)*1000:.2f}ms\")\n",
    "print(f\"  ì¤‘ì•™ê°’: {np.median(latencies_v1)*1000:.2f}ms\")\n",
    "print(f\"  p95: {np.percentile(latencies_v1, 95)*1000:.2f}ms\")\n",
    "print(f\"  p99: {np.percentile(latencies_v1, 99)*1000:.2f}ms\")\n",
    "\n",
    "print(\"\\nv2.0:\")\n",
    "print(f\"  í‰ê· : {np.mean(latencies_v2)*1000:.2f}ms\")\n",
    "print(f\"  ì¤‘ì•™ê°’: {np.median(latencies_v2)*1000:.2f}ms\")\n",
    "print(f\"  p95: {np.percentile(latencies_v2, 95)*1000:.2f}ms\")\n",
    "print(f\"  p99: {np.percentile(latencies_v2, 99)*1000:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. PromQL ì¿¼ë¦¬ ì‹¤ìŠµ\n",
    "\n",
    "Prometheus ì¿¼ë¦¬ ì–¸ì–´(PromQL)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ì¡°íšŒí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus ì—°ê²° ì„¤ì •\n",
    "PROMETHEUS_URL = \"http://localhost:9090\"  # Prometheus ì„œë²„ ì£¼ì†Œ\n",
    "\n",
    "def query_prometheus(query):\n",
    "    \"\"\"Prometheus ì¿¼ë¦¬ ì‹¤í–‰\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{PROMETHEUS_URL}/api/v1/query\",\n",
    "            params={'query': query}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {'error': f'HTTP {response.status_code}'}\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PromQL ì¿¼ë¦¬ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâš ï¸  ì£¼ì˜: Prometheusê°€ {PROMETHEUS_URL}ì—ì„œ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤\")\n",
    "print(\"\\nPrometheus ì—°ê²°:\")\n",
    "print(\"  kubectl port-forward -n monitoring-system svc/prometheus-server 9090:80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì œ ì¿¼ë¦¬ë“¤\n",
    "queries = [\n",
    "    (\"í˜„ì¬ MAE ê°’\", 'model_mae_score'),\n",
    "    (\"í˜„ì¬ RÂ² ê°’\", 'model_r2_score'),\n",
    "    (\"ì´ ì˜ˆì¸¡ ìˆ˜\", 'model_predictions_total'),\n",
    "    (\"ìµœê·¼ 5ë¶„ ì˜ˆì¸¡ ì¦ê°€ìœ¨\", 'rate(model_predictions_total[5m])'),\n",
    "    (\"p95 ë ˆì´í„´ì‹œ\", 'histogram_quantile(0.95, rate(model_prediction_latency_seconds_bucket[5m]))'),\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PromQL ì¿¼ë¦¬ ì˜ˆì œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for description, query in queries:\n",
    "    print(f\"\\nğŸ“Š {description}\")\n",
    "    print(f\"   Query: {query}\")\n",
    "    \n",
    "    result = query_prometheus(query)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"   âš ï¸  Error: {result['error']}\")\n",
    "    elif result.get('status') == 'success':\n",
    "        data = result.get('data', {}).get('result', [])\n",
    "        if data:\n",
    "            for item in data[:3]:  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ\n",
    "                labels = item.get('metric', {})\n",
    "                value = item.get('value', [None, 'N/A'])[1]\n",
    "                print(f\"   âœ… {labels}: {value}\")\n",
    "        else:\n",
    "            print(\"   â„¹ï¸  No data\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Unexpected response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜\n",
    "\n",
    "ë‘ ëª¨ë¸ ë²„ì „ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# íŠ¸ë˜í”½ ë¶„í•  ì„¤ì •\n",
    "traffic_split = {\n",
    "    'v1.0': 0.5,  # 50%\n",
    "    'v2.0': 0.5   # 50%\n",
    "}\n",
    "\n",
    "print(f\"\\níŠ¸ë˜í”½ ë¶„í• : v1.0={traffic_split['v1.0']*100:.0f}%, v2.0={traffic_split['v2.0']*100:.0f}%\")\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰\n",
    "n_requests = 200\n",
    "results = {'v1.0': [], 'v2.0': []}\n",
    "\n",
    "print(f\"\\n{n_requests}ê°œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜ ì¤‘...\")\n",
    "\n",
    "for i in range(n_requests):\n",
    "    # íŠ¸ë˜í”½ ë¶„í• \n",
    "    version = 'v1.0' if random.random() < traffic_split['v1.0'] else 'v2.0'\n",
    "    \n",
    "    # ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜\n",
    "    if version == 'v1.0':\n",
    "        mae = 0.445 + random.gauss(0, 0.01)\n",
    "        latency = abs(random.gauss(0.05, 0.02))\n",
    "        success = random.random() > 0.05  # 95% ì„±ê³µë¥ \n",
    "    else:  # v2.0\n",
    "        mae = 0.362 + random.gauss(0, 0.01)\n",
    "        latency = abs(random.gauss(0.04, 0.01))\n",
    "        success = random.random() > 0.02  # 98% ì„±ê³µë¥ \n",
    "    \n",
    "    # ê²°ê³¼ ê¸°ë¡\n",
    "    results[version].append({\n",
    "        'mae': mae,\n",
    "        'latency': latency,\n",
    "        'success': success\n",
    "    })\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\n",
    "    mae_gauge.labels(model_version=version).set(mae)\n",
    "    latency_histogram.labels(model_version=version).observe(latency)\n",
    "    status = 'success' if success else 'error'\n",
    "    prediction_counter.labels(model_version=version, status=status).inc()\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  ì§„í–‰: {i+1}/{n_requests}\")\n",
    "\n",
    "print(\"\\nâœ… ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„\n",
    "print(\"=\" * 60)\n",
    "print(\"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for version in ['v1.0', 'v2.0']:\n",
    "    data = results[version]\n",
    "    \n",
    "    maes = [d['mae'] for d in data]\n",
    "    latencies = [d['latency'] * 1000 for d in data]  # msë¡œ ë³€í™˜\n",
    "    success_rate = sum(d['success'] for d in data) / len(data) * 100\n",
    "    \n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"ëª¨ë¸ ë²„ì „: {version}\")\n",
    "    print(f\"{'='*30}\")\n",
    "    print(f\"ìš”ì²­ ìˆ˜: {len(data)}\")\n",
    "    print(f\"\\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:\")\n",
    "    print(f\"  MAE:\")\n",
    "    print(f\"    í‰ê· : {np.mean(maes):.4f}\")\n",
    "    print(f\"    í‘œì¤€í¸ì°¨: {np.std(maes):.4f}\")\n",
    "    print(f\"\\n  ë ˆì´í„´ì‹œ:\")\n",
    "    print(f\"    í‰ê· : {np.mean(latencies):.2f}ms\")\n",
    "    print(f\"    p50: {np.percentile(latencies, 50):.2f}ms\")\n",
    "    print(f\"    p95: {np.percentile(latencies, 95):.2f}ms\")\n",
    "    print(f\"    p99: {np.percentile(latencies, 99):.2f}ms\")\n",
    "    print(f\"\\n  ì„±ê³µë¥ : {success_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B í…ŒìŠ¤íŠ¸ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MAE ë¹„êµ\n",
    "maes_v1 = [d['mae'] for d in results['v1.0']]\n",
    "maes_v2 = [d['mae'] for d in results['v2.0']]\n",
    "axes[0, 0].boxplot([maes_v1, maes_v2], labels=['v1.0', 'v2.0'])\n",
    "axes[0, 0].set_title('MAE Comparison')\n",
    "axes[0, 0].set_ylabel('MAE')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Latency ë¹„êµ\n",
    "latencies_v1 = [d['latency'] * 1000 for d in results['v1.0']]\n",
    "latencies_v2 = [d['latency'] * 1000 for d in results['v2.0']]\n",
    "axes[0, 1].boxplot([latencies_v1, latencies_v2], labels=['v1.0', 'v2.0'])\n",
    "axes[0, 1].set_title('Latency Comparison')\n",
    "axes[0, 1].set_ylabel('Latency (ms)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Success Rate ë¹„êµ\n",
    "success_v1 = sum(d['success'] for d in results['v1.0']) / len(results['v1.0']) * 100\n",
    "success_v2 = sum(d['success'] for d in results['v2.0']) / len(results['v2.0']) * 100\n",
    "axes[1, 0].bar(['v1.0', 'v2.0'], [success_v1, success_v2], color=['blue', 'orange'])\n",
    "axes[1, 0].set_title('Success Rate Comparison')\n",
    "axes[1, 0].set_ylabel('Success Rate (%)')\n",
    "axes[1, 0].set_ylim([90, 100])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Request Distribution\n",
    "request_counts = [len(results['v1.0']), len(results['v2.0'])]\n",
    "axes[1, 1].pie(request_counts, labels=['v1.0', 'v2.0'], autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Request Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ìŠ¹ì ê²°ì •\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ† A/B í…ŒìŠ¤íŠ¸ ê²°ë¡ \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mae_improvement = (1 - np.mean(maes_v2) / np.mean(maes_v1)) * 100\n",
    "latency_improvement = (1 - np.mean(latencies_v2) / np.mean(latencies_v1)) * 100\n",
    "\n",
    "print(f\"\\nv2.0ì´ v1.0 ëŒ€ë¹„:\")\n",
    "print(f\"  ğŸ“Š MAE: {mae_improvement:+.2f}% ê°œì„ \")\n",
    "print(f\"  âš¡ ë ˆì´í„´ì‹œ: {latency_improvement:+.2f}% ê°œì„ \")\n",
    "print(f\"  âœ… ì„±ê³µë¥ : {success_v2 - success_v1:+.2f}%p ê°œì„ \")\n",
    "\n",
    "if mae_improvement > 0 and latency_improvement > 0:\n",
    "    print(\"\\nğŸ‰ ê²°ë¡ : v2.0ì„ í”„ë¡œë•ì…˜ì— ë°°í¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ê²°ë¡ : ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ì•Œë¦¼ ê·œì¹™ ì‹œë®¬ë ˆì´ì…˜\n",
    "\n",
    "ì„±ëŠ¥ ì„ê³„ê°’ì„ ì„¤ì •í•˜ê³  ì•Œë¦¼ ì¡°ê±´ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•Œë¦¼ ê·œì¹™ ì •ì˜\n",
    "alert_rules = {\n",
    "    'HighMAE': {\n",
    "        'threshold': 0.50,\n",
    "        'metric': 'mae',\n",
    "        'description': 'MAEê°€ 0.50ì„ ì´ˆê³¼'\n",
    "    },\n",
    "    'LowR2': {\n",
    "        'threshold': 0.75,\n",
    "        'metric': 'r2',\n",
    "        'description': 'RÂ²ê°€ 0.75 ë¯¸ë§Œ'\n",
    "    },\n",
    "    'HighLatency': {\n",
    "        'threshold': 100,  # ms\n",
    "        'metric': 'latency',\n",
    "        'description': 'ë ˆì´í„´ì‹œê°€ 100ms ì´ˆê³¼'\n",
    "    },\n",
    "    'LowSuccessRate': {\n",
    "        'threshold': 95,  # %\n",
    "        'metric': 'success_rate',\n",
    "        'description': 'ì„±ê³µë¥ ì´ 95% ë¯¸ë§Œ'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ì•Œë¦¼ ê·œì¹™ ì„¤ì •\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for alert_name, rule in alert_rules.items():\n",
    "    print(f\"\\nğŸ“¢ {alert_name}\")\n",
    "    print(f\"   ì¡°ê±´: {rule['description']}\")\n",
    "    print(f\"   ì„ê³„ê°’: {rule['threshold']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•Œë¦¼ ì²´í¬ í•¨ìˆ˜\n",
    "def check_alerts(results, alert_rules):\n",
    "    \"\"\"ì•Œë¦¼ ì¡°ê±´ ì²´í¬\"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    for version, data in results.items():\n",
    "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        maes = [d['mae'] for d in data]\n",
    "        latencies = [d['latency'] * 1000 for d in data]\n",
    "        success_rate = sum(d['success'] for d in data) / len(data) * 100\n",
    "        \n",
    "        metrics = {\n",
    "            'mae': np.mean(maes),\n",
    "            'latency': np.percentile(latencies, 95),  # p95\n",
    "            'success_rate': success_rate\n",
    "        }\n",
    "        \n",
    "        # ì•Œë¦¼ ì²´í¬\n",
    "        for alert_name, rule in alert_rules.items():\n",
    "            metric_name = rule['metric']\n",
    "            threshold = rule['threshold']\n",
    "            \n",
    "            if metric_name == 'r2':\n",
    "                continue  # RÂ² ë©”íŠ¸ë¦­ì€ ì—¬ê¸°ì„œ ê³„ì‚° ì•ˆí•¨\n",
    "            \n",
    "            value = metrics.get(metric_name)\n",
    "            \n",
    "            # ì•Œë¦¼ ì¡°ê±´ í™•ì¸\n",
    "            triggered = False\n",
    "            if metric_name == 'success_rate':\n",
    "                triggered = value < threshold\n",
    "            else:\n",
    "                triggered = value > threshold\n",
    "            \n",
    "            if triggered:\n",
    "                alerts.append({\n",
    "                    'alert': alert_name,\n",
    "                    'version': version,\n",
    "                    'metric': metric_name,\n",
    "                    'value': value,\n",
    "                    'threshold': threshold,\n",
    "                    'description': rule['description']\n",
    "                })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# ì•Œë¦¼ ì²´í¬ ì‹¤í–‰\n",
    "print(\"=\" * 60)\n",
    "print(\"ì•Œë¦¼ ì¡°ê±´ ì²´í¬\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "alerts = check_alerts(results, alert_rules)\n",
    "\n",
    "if alerts:\n",
    "    print(f\"\\nğŸš¨ {len(alerts)}ê°œ ì•Œë¦¼ ë°œìƒ!\\n\")\n",
    "    for alert in alerts:\n",
    "        print(f\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n",
    "        print(f\"ğŸ”” {alert['alert']}\")\n",
    "        print(f\"   ë²„ì „: {alert['version']}\")\n",
    "        print(f\"   ì¡°ê±´: {alert['description']}\")\n",
    "        print(f\"   í˜„ì¬ ê°’: {alert['value']:.2f}\")\n",
    "        print(f\"   ì„ê³„ê°’: {alert['threshold']}\")\n",
    "else:\n",
    "    print(\"\\nâœ… ëª¨ë“  ë©”íŠ¸ë¦­ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ì‹¤ìŠµ ì™„ë£Œ!\n",
    "\n",
    "### í•™ìŠµí•œ ë‚´ìš©\n",
    "\n",
    "- âœ… Prometheus ë©”íŠ¸ë¦­ íƒ€ì… (Counter, Gauge, Histogram, Summary)\n",
    "- âœ… Custom Metrics Exporter êµ¬í˜„\n",
    "- âœ… PromQL ì¿¼ë¦¬ ì‘ì„±\n",
    "- âœ… ë©”íŠ¸ë¦­ ì‹œê°í™” (Matplotlib)\n",
    "- âœ… A/B í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜\n",
    "- âœ… ì•Œë¦¼ ê·œì¹™ ì„¤ì • ë° í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "1. **Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±**\n",
    "   - ë©”íŠ¸ë¦­ì„ Grafanaì—ì„œ ì‹œê°í™”\n",
    "   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±\n",
    "\n",
    "2. **Alertmanager ì„¤ì •**\n",
    "   - ì•Œë¦¼ ë¼ìš°íŒ… êµ¬ì„±\n",
    "   - Slack/Email ì—°ë™\n",
    "\n",
    "3. **í”„ë¡œë•ì…˜ ë°°í¬**\n",
    "   - Kubernetesì— ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬\n",
    "   - ì‹¤ì œ ëª¨ë¸ ì„œë¹™ ì—°ë™\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [Prometheus Python Client](https://github.com/prometheus/client_python)\n",
    "- [PromQL Documentation](https://prometheus.io/docs/prometheus/latest/querying/basics/)\n",
    "- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)\n",
    "- [Alertmanager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)\n",
    "\n",
    "---\n",
    "\n",
    "Â© 2024 í˜„ëŒ€ì˜¤í† ì—ë²„ MLOps Training - Lab 3-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
