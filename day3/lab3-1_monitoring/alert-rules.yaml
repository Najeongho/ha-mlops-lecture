# ============================================================
# Lab 3-1: Prometheus Alert Rules
# MLOps ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì•Œë¦¼ ê·œì¹™
# ============================================================
#
# ì ìš© ë°©ë²•:
#   kubectl apply -f alert-rules.yaml
#
# ì°¸ê³ : Prometheus Operatorê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
# ============================================================

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mlops-training-alerts
  namespace: monitoring
  labels:
    app: kube-prometheus-stack
    release: prometheus
spec:
  groups:
    # ============================================================
    # Model Serving Alerts
    # ============================================================
    - name: model-serving
      rules:
        # ë†’ì€ ì—ëŸ¬ìœ¨ ì•Œë¦¼
        - alert: HighModelErrorRate
          expr: |
            (
              sum(rate(revision_request_count{response_code_class="5xx"}[5m])) by (namespace, service_name)
              /
              sum(rate(revision_request_count[5m])) by (namespace, service_name)
            ) > 0.05
          for: 5m
          labels:
            severity: warning
            team: mlops
          annotations:
            summary: "ë†’ì€ ëª¨ë¸ ì—ëŸ¬ìœ¨ ê°ì§€"
            description: "{{ $labels.namespace }}/{{ $labels.service_name }}ì˜ ì—ëŸ¬ìœ¨ì´ 5%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ê°’: {{ $value | humanizePercentage }}"
            runbook_url: "https://wiki.example.com/runbooks/model-error-rate"

        # ë§¤ìš° ë†’ì€ ì—ëŸ¬ìœ¨ ì•Œë¦¼ (Critical)
        - alert: CriticalModelErrorRate
          expr: |
            (
              sum(rate(revision_request_count{response_code_class="5xx"}[5m])) by (namespace, service_name)
              /
              sum(rate(revision_request_count[5m])) by (namespace, service_name)
            ) > 0.10
          for: 2m
          labels:
            severity: critical
            team: mlops
          annotations:
            summary: "ì‹¬ê°í•œ ëª¨ë¸ ì—ëŸ¬ìœ¨ ê°ì§€"
            description: "{{ $labels.namespace }}/{{ $labels.service_name }}ì˜ ì—ëŸ¬ìœ¨ì´ 10%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."

        # ë†’ì€ ì§€ì—° ì‹œê°„ ì•Œë¦¼
        - alert: HighModelLatency
          expr: |
            histogram_quantile(0.95, 
              sum(rate(revision_request_latencies_bucket[5m])) by (le, namespace, service_name)
            ) > 0.5
          for: 5m
          labels:
            severity: warning
            team: mlops
          annotations:
            summary: "ë†’ì€ ëª¨ë¸ ì‘ë‹µ ì§€ì—° ê°ì§€"
            description: "{{ $labels.namespace }}/{{ $labels.service_name }}ì˜ P95 ì‘ë‹µ ì‹œê°„ì´ 500msë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ê°’: {{ $value | humanizeDuration }}"

        # ë§¤ìš° ë†’ì€ ì§€ì—° ì‹œê°„ ì•Œë¦¼
        - alert: CriticalModelLatency
          expr: |
            histogram_quantile(0.95, 
              sum(rate(revision_request_latencies_bucket[5m])) by (le, namespace, service_name)
            ) > 2.0
          for: 2m
          labels:
            severity: critical
            team: mlops
          annotations:
            summary: "ì‹¬ê°í•œ ëª¨ë¸ ì‘ë‹µ ì§€ì—° ê°ì§€"
            description: "{{ $labels.namespace }}/{{ $labels.service_name }}ì˜ P95 ì‘ë‹µ ì‹œê°„ì´ 2ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

    # ============================================================
    # Pod Health Alerts
    # ============================================================
    - name: pod-health
      rules:
        # Pod Down ì•Œë¦¼
        - alert: ModelPodDown
          expr: |
            kube_pod_status_phase{phase="Running", namespace=~"kubeflow-user.*"} == 0
          for: 5m
          labels:
            severity: critical
            team: mlops
          annotations:
            summary: "ëª¨ë¸ Podê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ"
            description: "{{ $labels.namespace }}/{{ $labels.pod }}ê°€ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤."

        # Pod ì¬ì‹œì‘ ì•Œë¦¼
        - alert: ModelPodRestartingTooMuch
          expr: |
            increase(kube_pod_container_status_restarts_total{namespace=~"kubeflow-user.*"}[1h]) > 3
          for: 5m
          labels:
            severity: warning
            team: mlops
          annotations:
            summary: "Pod ì¬ì‹œì‘ ë¹ˆë„ ë†’ìŒ"
            description: "{{ $labels.namespace }}/{{ $labels.pod }}ê°€ ì§€ë‚œ 1ì‹œê°„ ë™ì•ˆ {{ $value }}íšŒ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."

        # Pod Pending ìƒíƒœ ì§€ì†
        - alert: ModelPodPending
          expr: |
            kube_pod_status_phase{phase="Pending", namespace=~"kubeflow-user.*"} == 1
          for: 15m
          labels:
            severity: warning
            team: mlops
          annotations:
            summary: "Podê°€ Pending ìƒíƒœë¡œ ì§€ì†"
            description: "{{ $labels.namespace }}/{{ $labels.pod }}ê°€ 15ë¶„ ì´ìƒ Pending ìƒíƒœì…ë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."

    # ============================================================
    # Resource Usage Alerts
    # ============================================================
    - name: resource-usage
      rules:
        # CPU ì‚¬ìš©ë¥  ë†’ìŒ
        - alert: HighCPUUsage
          expr: |
            (
              sum(rate(container_cpu_usage_seconds_total{namespace=~"kubeflow-user.*"}[5m])) by (namespace, pod)
              /
              sum(container_spec_cpu_quota{namespace=~"kubeflow-user.*"} / container_spec_cpu_period{namespace=~"kubeflow-user.*"}) by (namespace, pod)
            ) > 0.80
          for: 10m
          labels:
            severity: warning
            team: mlops
          annotations:
            summary: "ë†’ì€ CPU ì‚¬ìš©ë¥ "
            description: "{{ $labels.namespace }}/{{ $labels.pod }}ì˜ CPU ì‚¬ìš©ë¥ ì´ 80%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

        # Memory ì‚¬ìš©ë¥  ë†’ìŒ
        - alert: HighMemoryUsage
          expr: |
            (
              sum(container_memory_usage_bytes{namespace=~"kubeflow-user.*"}) by (namespace, pod)
              /
              sum(container_spec_memory_limit_bytes{namespace=~"kubeflow-user.*"}) by (namespace, pod)
            ) > 0.85
          for: 10m
          labels:
            severity: warning
            team: mlops
          annotations:
            summary: "ë†’ì€ Memory ì‚¬ìš©ë¥ "
            description: "{{ $labels.namespace }}/{{ $labels.pod }}ì˜ Memory ì‚¬ìš©ë¥ ì´ 85%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."

    # ============================================================
    # InferenceService Alerts
    # ============================================================
    - name: inferenceservice
      rules:
        # InferenceService Not Ready
        - alert: InferenceServiceNotReady
          expr: |
            kserve_inferenceservice_ready{namespace=~"kubeflow-user.*"} == 0
          for: 10m
          labels:
            severity: critical
            team: mlops
          annotations:
            summary: "InferenceServiceê°€ Ready ìƒíƒœê°€ ì•„ë‹˜"
            description: "{{ $labels.namespace }}/{{ $labels.name }}ê°€ 10ë¶„ ì´ìƒ Ready ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤."

        # íŠ¸ë˜í”½ ì—†ìŒ (ì„ íƒì )
        - alert: NoTrafficToModel
          expr: |
            sum(rate(revision_request_count{namespace=~"kubeflow-user.*"}[30m])) by (namespace, service_name) == 0
          for: 30m
          labels:
            severity: info
            team: mlops
          annotations:
            summary: "ëª¨ë¸ì— íŠ¸ë˜í”½ ì—†ìŒ"
            description: "{{ $labels.namespace }}/{{ $labels.service_name }}ì— 30ë¶„ê°„ íŠ¸ë˜í”½ì´ ì—†ìŠµë‹ˆë‹¤."

---
# ============================================================
# AlertmanagerConfig (ì„ íƒì )
# Slack ë˜ëŠ” Email ì•Œë¦¼ ì„¤ì •
# ============================================================

apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: mlops-alerts-config
  namespace: monitoring
  labels:
    alertmanagerConfig: mlops
spec:
  route:
    receiver: 'default-receiver'
    groupBy: ['alertname', 'namespace']
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 4h
    routes:
      - receiver: 'critical-receiver'
        matchers:
          - name: severity
            value: critical
        continue: true
      - receiver: 'warning-receiver'
        matchers:
          - name: severity
            value: warning

  receivers:
    - name: 'default-receiver'
      # ê¸°ë³¸ ìˆ˜ì‹ ì ì„¤ì • (ì‹¤ì œ í™˜ê²½ì—ì„œ êµ¬ì„±)
      
    - name: 'critical-receiver'
      # Slack Webhook ì˜ˆì‹œ (ì‹¤ì œ í™˜ê²½ì—ì„œ URL ì„¤ì •)
      # slackConfigs:
      #   - apiURL:
      #       name: slack-webhook
      #       key: url
      #     channel: '#mlops-alerts-critical'
      #     sendResolved: true
      #     title: 'ğŸš¨ Critical Alert'
      #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      
    - name: 'warning-receiver'
      # Slack Webhook ì˜ˆì‹œ
      # slackConfigs:
      #   - apiURL:
      #       name: slack-webhook
      #       key: url
      #     channel: '#mlops-alerts'
      #     sendResolved: true
      #     title: 'âš ï¸ Warning Alert'
      #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
