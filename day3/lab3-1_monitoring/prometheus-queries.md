# Lab 3-1: Prometheus ì¿¼ë¦¬ ëª¨ìŒ

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” MLOps ëª¨ë‹ˆí„°ë§ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” PromQL ì¿¼ë¦¬ë¥¼ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

## ğŸ”§ ê¸°ë³¸ ì¿¼ë¦¬

### 1. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­

#### CPU ì‚¬ìš©ëŸ‰

```promql
# Pod CPU ì‚¬ìš©ëŸ‰ (5ë¶„ í‰ê· )
sum(rate(container_cpu_usage_seconds_total{namespace="kubeflow-userXX"}[5m])) by (pod)

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ CPU ì‚¬ìš©ëŸ‰
sum(rate(container_cpu_usage_seconds_total{namespace="kubeflow-userXX"}[5m]))

# CPU ì‚¬ìš©ë¥  (%)
sum(rate(container_cpu_usage_seconds_total{namespace="kubeflow-userXX"}[5m])) by (pod) 
/ 
sum(container_spec_cpu_quota{namespace="kubeflow-userXX"} / container_spec_cpu_period{namespace="kubeflow-userXX"}) by (pod) 
* 100
```

#### Memory ì‚¬ìš©ëŸ‰

```promql
# Pod Memory ì‚¬ìš©ëŸ‰ (bytes)
sum(container_memory_usage_bytes{namespace="kubeflow-userXX"}) by (pod)

# Memory ì‚¬ìš©ëŸ‰ (MiB)
sum(container_memory_usage_bytes{namespace="kubeflow-userXX"}) by (pod) / 1024 / 1024

# Memory ì‚¬ìš©ë¥  (%)
sum(container_memory_usage_bytes{namespace="kubeflow-userXX"}) by (pod) 
/ 
sum(container_spec_memory_limit_bytes{namespace="kubeflow-userXX"}) by (pod) 
* 100
```

#### Network I/O

```promql
# ë„¤íŠ¸ì›Œí¬ ìˆ˜ì‹  (bytes/s)
sum(rate(container_network_receive_bytes_total{namespace="kubeflow-userXX"}[5m])) by (pod)

# ë„¤íŠ¸ì›Œí¬ ì†¡ì‹  (bytes/s)
sum(rate(container_network_transmit_bytes_total{namespace="kubeflow-userXX"}[5m])) by (pod)
```

---

## ğŸ“Š KServe / ëª¨ë¸ ì„œë¹™ ë©”íŠ¸ë¦­

### 2. ìš”ì²­ ê´€ë ¨ ë©”íŠ¸ë¦­

```promql
# ìš”ì²­ ìˆ˜ (RPS - Requests Per Second)
rate(revision_request_count{namespace="kubeflow-userXX"}[1m])

# ì´ ìš”ì²­ ìˆ˜
sum(increase(revision_request_count{namespace="kubeflow-userXX"}[1h]))

# ì„œë¹„ìŠ¤ë³„ ìš”ì²­ ìˆ˜
sum(rate(revision_request_count{namespace="kubeflow-userXX"}[5m])) by (service_name)
```

### 3. ì§€ì—° ì‹œê°„ (Latency)

```promql
# í‰ê·  ì‘ë‹µ ì‹œê°„ (ì´ˆ)
rate(revision_request_latencies_sum{namespace="kubeflow-userXX"}[5m]) 
/ 
rate(revision_request_latencies_count{namespace="kubeflow-userXX"}[5m])

# P95 ì‘ë‹µ ì‹œê°„
histogram_quantile(0.95, sum(rate(revision_request_latencies_bucket{namespace="kubeflow-userXX"}[5m])) by (le, service_name))

# P99 ì‘ë‹µ ì‹œê°„
histogram_quantile(0.99, sum(rate(revision_request_latencies_bucket{namespace="kubeflow-userXX"}[5m])) by (le, service_name))

# P50 ì‘ë‹µ ì‹œê°„ (ì¤‘ì•™ê°’)
histogram_quantile(0.50, sum(rate(revision_request_latencies_bucket{namespace="kubeflow-userXX"}[5m])) by (le))
```

### 4. ì—ëŸ¬ìœ¨

```promql
# HTTP 5xx ì—ëŸ¬ìœ¨ (%)
sum(rate(revision_request_count{namespace="kubeflow-userXX", response_code_class="5xx"}[5m])) 
/ 
sum(rate(revision_request_count{namespace="kubeflow-userXX"}[5m])) 
* 100

# HTTP 4xx ì—ëŸ¬ìœ¨ (%)
sum(rate(revision_request_count{namespace="kubeflow-userXX", response_code_class="4xx"}[5m])) 
/ 
sum(rate(revision_request_count{namespace="kubeflow-userXX"}[5m])) 
* 100

# ì„±ê³µë¥  (%)
sum(rate(revision_request_count{namespace="kubeflow-userXX", response_code_class="2xx"}[5m])) 
/ 
sum(rate(revision_request_count{namespace="kubeflow-userXX"}[5m])) 
* 100
```

---

## ğŸ¯ InferenceService ì „ìš© ë©”íŠ¸ë¦­

### 5. KServe ë©”íŠ¸ë¦­

```promql
# InferenceService ìš”ì²­ ìˆ˜
sum(rate(kserve_inference_request_total{namespace="kubeflow-userXX"}[5m])) by (model_name)

# ì¶”ë¡  ì§€ì—° ì‹œê°„
histogram_quantile(0.95, sum(rate(kserve_inference_request_duration_seconds_bucket{namespace="kubeflow-userXX"}[5m])) by (le, model_name))

# ëª¨ë¸ë³„ ì—ëŸ¬ ìˆ˜
sum(increase(kserve_inference_request_total{namespace="kubeflow-userXX", status="error"}[1h])) by (model_name)
```

---

## ğŸ”” ì•Œë¦¼ìš© ì¿¼ë¦¬

### 6. ì•Œë¦¼ ì¡°ê±´

```promql
# ë†’ì€ ì—ëŸ¬ìœ¨ (> 5%)
sum(rate(revision_request_count{namespace="kubeflow-userXX", response_code_class="5xx"}[5m])) 
/ 
sum(rate(revision_request_count{namespace="kubeflow-userXX"}[5m])) 
> 0.05

# ë†’ì€ ì§€ì—° ì‹œê°„ (P95 > 500ms)
histogram_quantile(0.95, sum(rate(revision_request_latencies_bucket{namespace="kubeflow-userXX"}[5m])) by (le)) 
> 0.5

# Pod ì¬ì‹œì‘ (1ì‹œê°„ ë‚´)
increase(kube_pod_container_status_restarts_total{namespace="kubeflow-userXX"}[1h]) > 3

# Pod Down
kube_pod_status_phase{namespace="kubeflow-userXX", phase="Running"} == 0
```

---

## ğŸ“ˆ ëŒ€ì‹œë³´ë“œìš© ì¿¼ë¦¬

### 7. ì¢…í•© ë©”íŠ¸ë¦­

```promql
# ì‹¤í–‰ ì¤‘ì¸ Pod ìˆ˜
count(kube_pod_status_phase{namespace="kubeflow-userXX", phase="Running"})

# Ready InferenceService ìˆ˜
count(kube_customresource_inferenceservice_status{namespace="kubeflow-userXX", status="Ready"})

# ì´ CPU ìš”ì²­ëŸ‰
sum(kube_pod_container_resource_requests{namespace="kubeflow-userXX", resource="cpu"})

# ì´ Memory ìš”ì²­ëŸ‰ (GiB)
sum(kube_pod_container_resource_requests{namespace="kubeflow-userXX", resource="memory"}) / 1024 / 1024 / 1024
```

---

## ğŸ’¡ ì‚¬ìš© íŒ

### Grafanaì—ì„œ ë³€ìˆ˜ ì‚¬ìš©

ëŒ€ì‹œë³´ë“œì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ë©´ ì¿¼ë¦¬ë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```promql
# ë³€ìˆ˜ ì •ì˜
label_values(kube_namespace_labels, namespace)

# ì¿¼ë¦¬ì—ì„œ ë³€ìˆ˜ ì‚¬ìš©
sum(rate(container_cpu_usage_seconds_total{namespace="$namespace"}[5m])) by (pod)
```

### ì‹œê°„ ë²”ìœ„ ì„ íƒ

- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: `[1m]` ~ `[5m]`
- **íŠ¸ë Œë“œ ë¶„ì„**: `[1h]` ~ `[24h]`
- **ìš©ëŸ‰ ê³„íš**: `[7d]` ~ `[30d]`

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [PromQL ê³µì‹ ë¬¸ì„œ](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Prometheus í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤](https://prometheus.io/docs/prometheus/latest/querying/functions/)
- [KServe ë©”íŠ¸ë¦­ ë¬¸ì„œ](https://kserve.github.io/website/latest/modelserving/observability/prometheus/)
