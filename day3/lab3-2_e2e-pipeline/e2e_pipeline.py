"""
Lab 3-2: End-to-End ML Pipeline
================================

Îç∞Ïù¥ÌÑ∞ Î°úÎìú ‚Üí Ï†ÑÏ≤òÎ¶¨ ‚Üí ÌïôÏäµ ‚Üí ÌèâÍ∞Ä ‚Üí Î∞∞Ìè¨ÍπåÏßÄ
ÏôÑÏ†Ñ ÏûêÎèôÌôîÎêú MLOps ÌååÏù¥ÌîÑÎùºÏù∏

Ïã§Ìñâ:
    python e2e_pipeline.py
"""

import kfp
from kfp import dsl
from kfp.components import create_component_from_func
from kfp import compiler


# ============================================================
# Component 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
# ============================================================

@create_component_from_func
def load_data(
    data_source: str = "sklearn"
) -> str:
    """
    Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† Ï†ÄÏû•Ìï©ÎãàÎã§.
    
    Args:
        data_source: Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ ("sklearn" ÎòêÎäî S3 Í≤ΩÎ°ú)
    
    Returns:
        Ï†ÄÏû•Îêú Îç∞Ïù¥ÌÑ∞ ÌååÏùº Í≤ΩÎ°ú
    """
    import pandas as pd
    from sklearn.datasets import fetch_california_housing
    
    print("=" * 50)
    print("  Step 1: Load Data")
    print("=" * 50)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    if data_source == "sklearn":
        data = fetch_california_housing()
        df = pd.DataFrame(data.data, columns=data.feature_names)
        df['target'] = data.target
    else:
        # S3ÏóêÏÑú Î°úÎìúÌïòÎäî Í≤ΩÏö∞
        df = pd.read_csv(data_source)
    
    # Ï†ÄÏû•
    output_path = "/tmp/raw_data.csv"
    df.to_csv(output_path, index=False)
    
    print(f"  ‚úÖ Data loaded: {len(df)} rows, {len(df.columns)} columns")
    print(f"  ‚úÖ Saved to: {output_path}")
    
    return output_path


# ============================================================
# Component 2: Ï†ÑÏ≤òÎ¶¨
# ============================================================

@create_component_from_func
def preprocess(
    data_path: str,
    test_size: float = 0.2
) -> str:
    """
    Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î∞è Train/Test Î∂ÑÌï†
    
    Args:
        data_path: ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
        test_size: ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏ ÎπÑÏú®
    
    Returns:
        Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
    """
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import json
    
    print("=" * 50)
    print("  Step 2: Preprocess")
    print("=" * 50)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    df = pd.read_csv(data_path)
    print(f"  Loaded {len(df)} rows")
    
    # ÌîºÏ≤òÏôÄ ÌÉÄÍ≤ü Î∂ÑÎ¶¨
    X = df.drop('target', axis=1)
    y = df['target']
    
    # Train/Test Î∂ÑÌï†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # Ï†ïÍ∑úÌôî
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Ï†ÄÏû•
    output_dir = "/tmp/processed"
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    np.save(f"{output_dir}/X_train.npy", X_train_scaled)
    np.save(f"{output_dir}/X_test.npy", X_test_scaled)
    np.save(f"{output_dir}/y_train.npy", y_train.values)
    np.save(f"{output_dir}/y_test.npy", y_test.values)
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
    metadata = {
        "n_train": len(X_train),
        "n_test": len(X_test),
        "n_features": X_train.shape[1],
        "feature_names": list(X.columns)
    }
    with open(f"{output_dir}/metadata.json", "w") as f:
        json.dump(metadata, f)
    
    print(f"  ‚úÖ Train: {len(X_train)}, Test: {len(X_test)}")
    print(f"  ‚úÖ Saved to: {output_dir}")
    
    return output_dir


# ============================================================
# Component 3: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ
# ============================================================

@create_component_from_func
def feature_engineering(
    data_dir: str
) -> str:
    """
    ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ ÏàòÌñâ
    
    Args:
        data_dir: Ï†ÑÏ≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨
    
    Returns:
        ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ ÏôÑÎ£åÎêú Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨
    """
    import numpy as np
    import json
    
    print("=" * 50)
    print("  Step 3: Feature Engineering")
    print("=" * 50)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    X_train = np.load(f"{data_dir}/X_train.npy")
    X_test = np.load(f"{data_dir}/X_test.npy")
    
    with open(f"{data_dir}/metadata.json", "r") as f:
        metadata = json.load(f)
    
    feature_names = metadata["feature_names"]
    
    # ÌîºÏ≤ò Ïù∏Îç±Ïä§ Ï∞æÍ∏∞
    # California Housing: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Lat, Long
    rooms_idx = feature_names.index("AveRooms") if "AveRooms" in feature_names else 2
    bedrms_idx = feature_names.index("AveBedrms") if "AveBedrms" in feature_names else 3
    
    # ÏÉà ÌîºÏ≤ò: Î∞©Îãπ Ïπ®Ïã§ ÎπÑÏú®
    bedroom_ratio_train = X_train[:, bedrms_idx] / (X_train[:, rooms_idx] + 1e-6)
    bedroom_ratio_test = X_test[:, bedrms_idx] / (X_test[:, rooms_idx] + 1e-6)
    
    # ÌîºÏ≤ò Ï∂îÍ∞Ä
    X_train_new = np.column_stack([X_train, bedroom_ratio_train])
    X_test_new = np.column_stack([X_test, bedroom_ratio_test])
    
    # Ï†ÄÏû•
    output_dir = "/tmp/featured"
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    np.save(f"{output_dir}/X_train.npy", X_train_new)
    np.save(f"{output_dir}/X_test.npy", X_test_new)
    
    # y Îç∞Ïù¥ÌÑ∞ Î≥µÏÇ¨
    import shutil
    shutil.copy(f"{data_dir}/y_train.npy", f"{output_dir}/y_train.npy")
    shutil.copy(f"{data_dir}/y_test.npy", f"{output_dir}/y_test.npy")
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏
    metadata["feature_names"].append("bedroom_ratio")
    metadata["n_features"] += 1
    with open(f"{output_dir}/metadata.json", "w") as f:
        json.dump(metadata, f)
    
    print(f"  ‚úÖ Added feature: bedroom_ratio")
    print(f"  ‚úÖ New shape: {X_train_new.shape}")
    print(f"  ‚úÖ Saved to: {output_dir}")
    
    return output_dir


# ============================================================
# Component 4: Î™®Îç∏ ÌïôÏäµ + MLflow
# ============================================================

@create_component_from_func
def train_model(
    data_dir: str,
    mlflow_tracking_uri: str,
    experiment_name: str = "e2e-pipeline",
    n_estimators: int = 100,
    max_depth: int = 10
) -> str:
    """
    Î™®Îç∏ ÌïôÏäµ Î∞è MLflowÏóê Í∏∞Î°ù
    
    Args:
        data_dir: ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÎîîÎ†âÌÜ†Î¶¨
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
        experiment_name: Ïã§Ìóò Ïù¥Î¶Ñ
        n_estimators: Ìä∏Î¶¨ Í∞úÏàò
        max_depth: ÏµúÎåÄ ÍπäÏù¥
    
    Returns:
        MLflow Run ID
    """
    import numpy as np
    import mlflow
    import mlflow.sklearn
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error, r2_score
    import os
    
    print("=" * 50)
    print("  Step 4: Train Model")
    print("=" * 50)
    
    # ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    X_train = np.load(f"{data_dir}/X_train.npy")
    X_test = np.load(f"{data_dir}/X_test.npy")
    y_train = np.load(f"{data_dir}/y_train.npy")
    y_test = np.load(f"{data_dir}/y_test.npy")
    
    print(f"  Train: {X_train.shape}, Test: {X_test.shape}")
    
    # MLflow ÏÑ§Ï†ï
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    mlflow.set_experiment(experiment_name)
    
    # ÌïôÏäµ Î∞è Í∏∞Î°ù
    with mlflow.start_run() as run:
        run_id = run.info.run_id
        
        # ÌååÎùºÎØ∏ÌÑ∞ Í∏∞Î°ù
        mlflow.log_params({
            "n_estimators": n_estimators,
            "max_depth": max_depth,
            "random_state": 42
        })
        
        # Î™®Îç∏ ÌïôÏäµ
        model = RandomForestRegressor(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        
        # ÌèâÍ∞Ä
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # Î©îÌä∏Î¶≠ Í∏∞Î°ù
        mlflow.log_metrics({
            "mse": mse,
            "rmse": np.sqrt(mse),
            "r2": r2
        })
        
        # Î™®Îç∏ Ï†ÄÏû•
        mlflow.sklearn.log_model(
            model, "model",
            registered_model_name="e2e-california-model"
        )
        
        print(f"  ‚úÖ Model trained!")
        print(f"  ‚úÖ R2: {r2:.4f}, RMSE: {np.sqrt(mse):.4f}")
        print(f"  ‚úÖ Run ID: {run_id}")
    
    return run_id


# ============================================================
# Component 5: Î™®Îç∏ ÌèâÍ∞Ä (Ï°∞Í±¥ Î∂ÑÍ∏∞Ïö©)
# ============================================================

@create_component_from_func
def evaluate_model(
    run_id: str,
    mlflow_tracking_uri: str,
    r2_threshold: float = 0.8
) -> str:
    """
    Î™®Îç∏ ÏÑ±Îä• ÌèâÍ∞Ä Î∞è Î∞∞Ìè¨ Í≤∞Ï†ï
    
    Args:
        run_id: MLflow Run ID
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
        r2_threshold: R2 ÏûÑÍ≥ÑÍ∞í
    
    Returns:
        "deploy" ÎòêÎäî "skip"
    """
    import mlflow
    import os
    
    print("=" * 50)
    print("  Step 5: Evaluate Model")
    print("=" * 50)
    
    os.environ['MLFLOW_TRACKING_URI'] = mlflow_tracking_uri
    mlflow.set_tracking_uri(mlflow_tracking_uri)
    
    # Run Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
    client = mlflow.tracking.MlflowClient()
    run = client.get_run(run_id)
    
    r2 = float(run.data.metrics.get("r2", 0))
    
    print(f"  Run ID: {run_id}")
    print(f"  R2 Score: {r2:.4f}")
    print(f"  Threshold: {r2_threshold}")
    
    if r2 >= r2_threshold:
        decision = "deploy"
        print(f"  ‚úÖ Decision: DEPLOY (R2 >= threshold)")
    else:
        decision = "skip"
        print(f"  ‚ö†Ô∏è  Decision: SKIP (R2 < threshold)")
    
    return decision


# ============================================================
# Component 6: KServe Î∞∞Ìè¨
# ============================================================

@create_component_from_func
def deploy_model(
    run_id: str,
    model_name: str,
    namespace: str,
    mlflow_tracking_uri: str
):
    """
    KServe InferenceServiceÎ°ú Î™®Îç∏ Î∞∞Ìè¨
    
    Args:
        run_id: MLflow Run ID
        model_name: Î™®Îç∏ Ïù¥Î¶Ñ
        namespace: Kubernetes ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
    """
    from kubernetes import client, config
    import os
    
    print("=" * 50)
    print("  Step 6: Deploy Model")
    print("=" * 50)
    
    # Kubernetes ÏÑ§Ï†ï
    try:
        config.load_incluster_config()
    except:
        config.load_kube_config()
    
    # InferenceService Ï†ïÏùò
    isvc = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": model_name,
            "namespace": namespace,
            "annotations": {
                "mlflow.run_id": run_id
            }
        },
        "spec": {
            "predictor": {
                "sklearn": {
                    "storageUri": f"s3://mlflow-artifacts/{run_id}/artifacts/model",
                    "resources": {
                        "requests": {
                            "cpu": "100m",
                            "memory": "256Mi"
                        },
                        "limits": {
                            "cpu": "500m",
                            "memory": "512Mi"
                        }
                    }
                }
            }
        }
    }
    
    # Î∞∞Ìè¨
    api = client.CustomObjectsApi()
    
    try:
        # Í∏∞Ï°¥ Î¶¨ÏÜåÏä§ ÏÇ≠Ï†ú (ÏûàÏúºÎ©¥)
        api.delete_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            name=model_name
        )
        print(f"  ‚ö†Ô∏è  Deleted existing InferenceService: {model_name}")
    except:
        pass
    
    # ÏÉàÎ°ú ÏÉùÏÑ±
    api.create_namespaced_custom_object(
        group="serving.kserve.io",
        version="v1beta1",
        namespace=namespace,
        plural="inferenceservices",
        body=isvc
    )
    
    print(f"  ‚úÖ InferenceService created: {model_name}")
    print(f"  ‚úÖ Namespace: {namespace}")
    print(f"  ‚úÖ Run ID: {run_id}")


# ============================================================
# Component 7: ÏïåÎ¶º (Î∞∞Ìè¨ Ïä§ÌÇµ Ïãú)
# ============================================================

@create_component_from_func
def send_alert(
    run_id: str,
    message: str = "Model did not meet performance threshold"
):
    """
    ÏÑ±Îä• ÎØ∏Îã¨ Ïãú ÏïåÎ¶º Î∞úÏÜ°
    
    Args:
        run_id: MLflow Run ID
        message: ÏïåÎ¶º Î©îÏãúÏßÄ
    """
    print("=" * 50)
    print("  Step 6 (Alt): Send Alert")
    print("=" * 50)
    
    print(f"  ‚ö†Ô∏è  ALERT: {message}")
    print(f"  Run ID: {run_id}")
    print(f"  Action: Please review the model and retrain if needed")
    
    # Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Slack, Email Îì±ÏúºÎ°ú ÏïåÎ¶º Î∞úÏÜ°
    # import requests
    # requests.post(webhook_url, json={"text": message})


# ============================================================
# Pipeline Definition
# ============================================================

@dsl.pipeline(
    name='E2E ML Pipeline',
    description='End-to-End Machine Learning Pipeline with MLflow and KServe'
)
def e2e_ml_pipeline(
    data_source: str = "sklearn",
    mlflow_tracking_uri: str = "http://mlflow-server-service.mlflow-system.svc.cluster.local:5000",
    experiment_name: str = "e2e-pipeline",
    model_name: str = "california-model",
    namespace: str = "kubeflow-user01",
    n_estimators: int = 100,
    max_depth: int = 10,
    r2_threshold: float = 0.8
):
    """
    E2E ML Pipeline
    
    Îç∞Ïù¥ÌÑ∞ Î°úÎìú ‚Üí Ï†ÑÏ≤òÎ¶¨ ‚Üí ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ ‚Üí ÌïôÏäµ ‚Üí ÌèâÍ∞Ä ‚Üí Î∞∞Ìè¨
    
    Args:
        data_source: Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§
        mlflow_tracking_uri: MLflow ÏÑúÎ≤Ñ URI
        experiment_name: MLflow Ïã§Ìóò Ïù¥Î¶Ñ
        model_name: Î∞∞Ìè¨Ìï† Î™®Îç∏ Ïù¥Î¶Ñ
        namespace: Kubernetes ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§
        n_estimators: RandomForest Ìä∏Î¶¨ Í∞úÏàò
        max_depth: RandomForest ÏµúÎåÄ ÍπäÏù¥
        r2_threshold: Î∞∞Ìè¨ Í≤∞Ï†ï R2 ÏûÑÍ≥ÑÍ∞í
    """
    
    # Step 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    load_task = load_data(data_source=data_source)
    
    # Step 2: Ï†ÑÏ≤òÎ¶¨
    preprocess_task = preprocess(data_path=load_task.output)
    
    # Step 3: ÌîºÏ≤ò ÏóîÏßÄÎãàÏñ¥ÎßÅ
    feature_task = feature_engineering(data_dir=preprocess_task.output)
    
    # Step 4: Î™®Îç∏ ÌïôÏäµ
    train_task = train_model(
        data_dir=feature_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri,
        experiment_name=experiment_name,
        n_estimators=n_estimators,
        max_depth=max_depth
    )
    
    # Step 5: ÌèâÍ∞Ä
    evaluate_task = evaluate_model(
        run_id=train_task.output,
        mlflow_tracking_uri=mlflow_tracking_uri,
        r2_threshold=r2_threshold
    )
    
    # Step 6: Ï°∞Í±¥ Î∂ÑÍ∏∞ - Î∞∞Ìè¨ ÎòêÎäî ÏïåÎ¶º
    with dsl.Condition(evaluate_task.output == "deploy"):
        deploy_model(
            run_id=train_task.output,
            model_name=model_name,
            namespace=namespace,
            mlflow_tracking_uri=mlflow_tracking_uri
        )
    
    with dsl.Condition(evaluate_task.output == "skip"):
        send_alert(
            run_id=train_task.output,
            message="Model R2 score below threshold"
        )


# ============================================================
# Main
# ============================================================

if __name__ == '__main__':
    # ÌååÏù¥ÌîÑÎùºÏù∏ Ïª¥ÌååÏùº
    print("=" * 60)
    print("  Compiling E2E Pipeline...")
    print("=" * 60)
    
    pipeline_file = 'e2e_pipeline.yaml'
    compiler.Compiler().compile(
        pipeline_func=e2e_ml_pipeline,
        package_path=pipeline_file
    )
    print(f"‚úÖ Pipeline compiled: {pipeline_file}")
    
    # ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ
    try:
        print("\n" + "=" * 60)
        print("  Submitting Pipeline...")
        print("=" * 60)
        
        client = kfp.Client()
        
        run = client.create_run_from_pipeline_func(
            e2e_ml_pipeline,
            arguments={
                'data_source': 'sklearn',
                'experiment_name': 'e2e-pipeline',
                'model_name': 'california-model',
                'namespace': 'kubeflow-user01',  # ÏûêÏã†Ïùò ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§Î°ú Î≥ÄÍ≤Ω!
                'n_estimators': 100,
                'max_depth': 10,
                'r2_threshold': 0.75
            },
            experiment_name='e2e-experiment',
            run_name='e2e-run-001'
        )
        
        print(f"‚úÖ Pipeline submitted!")
        print(f"   Run ID: {run.run_id}")
        print("\nüí° Check the Kubeflow Dashboard ‚Üí Runs")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not submit pipeline: {e}")
        print(f"\n‚úÖ Pipeline YAML file created: {pipeline_file}")
        print("   Upload via Kubeflow UI ‚Üí Pipelines ‚Üí Upload Pipeline")
