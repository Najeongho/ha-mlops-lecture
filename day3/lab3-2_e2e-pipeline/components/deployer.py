"""
Lab 3-2: ëª¨ë¸ ë°°í¬ ì»´í¬ë„ŒíŠ¸
==========================

KServe InferenceServiceë¡œ ëª¨ë¸ì„ ë°°í¬í•˜ëŠ” ì»´í¬ë„ŒíŠ¸
"""

from kfp.components import create_component_from_func


@create_component_from_func
def deploy_model(
    run_id: str,
    model_name: str,
    namespace: str,
    mlflow_tracking_uri: str
):
    """
    KServe InferenceServiceë¡œ ëª¨ë¸ ë°°í¬
    
    Args:
        run_id: MLflow Run ID
        model_name: ëª¨ë¸ ì´ë¦„
        namespace: Kubernetes ë„¤ì„ìŠ¤í˜ì´ìŠ¤
        mlflow_tracking_uri: MLflow ì„œë²„ URI
    """
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import os
    import time
    
    print("=" * 50)
    print("  Component: Deploy Model")
    print("=" * 50)
    
    # Kubernetes ì„¤ì •
    print("\n  ğŸ”§ Kubernetes ì„¤ì • ì¤‘...")
    try:
        config.load_incluster_config()
        print("  âœ… In-cluster config ë¡œë“œë¨")
    except:
        config.load_kube_config()
        print("  âœ… Kube config ë¡œë“œë¨")
    
    # InferenceService ì •ì˜
    model_uri = f"s3://mlflow-artifacts/{run_id}/artifacts/model"
    
    print(f"\n  ğŸ“¦ ë°°í¬ ì •ë³´:")
    print(f"     - Model Name: {model_name}")
    print(f"     - Namespace: {namespace}")
    print(f"     - Model URI: {model_uri}")
    print(f"     - Run ID: {run_id}")
    
    isvc = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": model_name,
            "namespace": namespace,
            "labels": {
                "app": model_name,
                "mlflow-run-id": run_id
            },
            "annotations": {
                "autoscaling.knative.dev/minScale": "1",
                "autoscaling.knative.dev/maxScale": "3"
            }
        },
        "spec": {
            "predictor": {
                "sklearn": {
                    "storageUri": model_uri,
                    "resources": {
                        "requests": {
                            "cpu": "100m",
                            "memory": "256Mi"
                        },
                        "limits": {
                            "cpu": "500m",
                            "memory": "512Mi"
                        }
                    }
                }
            }
        }
    }
    
    # ë°°í¬
    api = client.CustomObjectsApi()
    
    # ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ì‚­ì œ (ìˆìœ¼ë©´)
    print("\n  ğŸ—‘ï¸ ê¸°ì¡´ InferenceService í™•ì¸...")
    try:
        api.delete_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            name=model_name
        )
        print(f"  âš ï¸ ê¸°ì¡´ InferenceService '{model_name}' ì‚­ì œë¨")
        time.sleep(5)  # ì‚­ì œ ì™„ë£Œ ëŒ€ê¸°
    except ApiException as e:
        if e.status == 404:
            print("  âœ… ê¸°ì¡´ InferenceService ì—†ìŒ")
        else:
            raise
    
    # ìƒˆë¡œ ìƒì„±
    print("\n  ğŸš€ InferenceService ìƒì„± ì¤‘...")
    try:
        result = api.create_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            body=isvc
        )
        print(f"  âœ… InferenceService ìƒì„±ë¨: {model_name}")
        
    except ApiException as e:
        print(f"  âŒ ë°°í¬ ì‹¤íŒ¨: {e.reason}")
        raise
    
    # ìƒíƒœ í™•ì¸ (ê°„ë‹¨íˆ)
    print("\n  â³ ë°°í¬ ìƒíƒœ í™•ì¸ ì¤‘... (ìµœëŒ€ 60ì´ˆ ëŒ€ê¸°)")
    for i in range(6):
        time.sleep(10)
        try:
            isvc_status = api.get_namespaced_custom_object(
                group="serving.kserve.io",
                version="v1beta1",
                namespace=namespace,
                plural="inferenceservices",
                name=model_name
            )
            
            conditions = isvc_status.get("status", {}).get("conditions", [])
            ready_condition = next(
                (c for c in conditions if c.get("type") == "Ready"),
                None
            )
            
            if ready_condition and ready_condition.get("status") == "True":
                print(f"  âœ… InferenceService READY!")
                break
            else:
                status = ready_condition.get("status", "Unknown") if ready_condition else "Unknown"
                print(f"  â³ Status: {status} ({(i+1)*10}s)")
                
        except Exception as e:
            print(f"  âš ï¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    # ì—”ë“œí¬ì¸íŠ¸ ì •ë³´
    print(f"\n  ğŸ“¡ ì—”ë“œí¬ì¸íŠ¸:")
    print(f"     http://{model_name}.{namespace}.svc.cluster.local/v1/models/{model_name}:predict")
    
    print(f"\n  âœ… ë°°í¬ ì™„ë£Œ!")


# ì»´í¬ë„ŒíŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    deploy_model.python_func(
        run_id="test-run-id",
        model_name="test-model",
        namespace="kubeflow-user01",
        mlflow_tracking_uri="http://localhost:5000"
    )
