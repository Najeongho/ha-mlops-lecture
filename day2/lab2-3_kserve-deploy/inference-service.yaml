# Lab 2-3: KServe InferenceService
# =================================
#
# 사용법:
#   1. ${NAMESPACE}를 자신의 네임스페이스로 변경
#   2. ${S3_MODEL_PATH}를 실제 모델 경로로 변경
#   3. kubectl apply -f inference-service.yaml

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: california-model
  namespace: ${NAMESPACE}  # 예: kubeflow-user01
  annotations:
    # 오토스케일링 설정
    autoscaling.knative.dev/minScale: "1"
    autoscaling.knative.dev/maxScale: "3"
    autoscaling.knative.dev/target: "10"
spec:
  predictor:
    # 모델 서버 타임아웃
    timeout: 60
    
    # Sklearn 모델 서버
    sklearn:
      # 모델 저장 위치 (S3 URI)
      storageUri: "s3://mlops-training-models/california/model"
      
      # 리소스 설정
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
      
      # 환경 변수 (선택)
      env:
      - name: LOG_LEVEL
        value: "INFO"
