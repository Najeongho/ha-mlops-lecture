"""
Lab 2-3: ëª¨ë¸ ì¤€ë¹„ ë° S3 ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
========================================

KServe ë°°í¬ë¥¼ ìœ„í•´ í•™ìŠµëœ ëª¨ë¸ì„ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python prepare_model.py [--bucket mlops-training-models] [--model-name california-model]
"""

import argparse
import os
import joblib
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


def train_model():
    """ëª¨ë¸ í•™ìŠµ"""
    print("=" * 60)
    print("  Step 1: ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    print("\n  ë°ì´í„° ë¡œë“œ ì¤‘...")
    data = fetch_california_housing()
    X, y = data.data, data.target
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"  - í•™ìŠµ ìƒ˜í”Œ: {len(X_train)}")
    print(f"  - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(X_test)}")
    
    # ëª¨ë¸ í•™ìŠµ
    print("\n  ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # í‰ê°€
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    print(f"\n  âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print(f"  - R2 Score: {r2:.4f}")
    print(f"  - RMSE: {rmse:.4f}")
    
    return model


def save_model_local(model, output_dir: str = "model"):
    """ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥"""
    print("\n" + "=" * 60)
    print("  Step 2: ëª¨ë¸ ë¡œì»¬ ì €ì¥")
    print("=" * 60)
    
    os.makedirs(output_dir, exist_ok=True)
    
    model_path = os.path.join(output_dir, "model.joblib")
    joblib.dump(model, model_path)
    
    print(f"\n  âœ… ëª¨ë¸ ì €ì¥: {model_path}")
    print(f"  - íŒŒì¼ í¬ê¸°: {os.path.getsize(model_path) / 1024:.1f} KB")
    
    return model_path


def upload_to_s3(local_path: str, bucket: str, model_name: str):
    """S3ì— ëª¨ë¸ ì—…ë¡œë“œ"""
    print("\n" + "=" * 60)
    print("  Step 3: S3 ì—…ë¡œë“œ")
    print("=" * 60)
    
    try:
        import boto3
        
        s3_client = boto3.client('s3')
        
        # S3 ê²½ë¡œ
        s3_key = f"{model_name}/model/model.joblib"
        
        print(f"\n  ì—…ë¡œë“œ ì¤‘...")
        print(f"  - ì†ŒìŠ¤: {local_path}")
        print(f"  - ëŒ€ìƒ: s3://{bucket}/{s3_key}")
        
        s3_client.upload_file(local_path, bucket, s3_key)
        
        print(f"\n  âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ!")
        
        # ì—…ë¡œë“œ í™•ì¸
        response = s3_client.head_object(Bucket=bucket, Key=s3_key)
        print(f"  - íŒŒì¼ í¬ê¸°: {response['ContentLength'] / 1024:.1f} KB")
        print(f"  - Last Modified: {response['LastModified']}")
        
        return f"s3://{bucket}/{model_name}/model"
        
    except ImportError:
        print("\n  âŒ boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("     pip install boto3")
        return None
    except Exception as e:
        print(f"\n  âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("\n  ğŸ’¡ AWS ìê²© ì¦ëª…ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("     aws configure")
        return None


def generate_inference_yaml(model_name: str, s3_uri: str, namespace: str):
    """InferenceService YAML ìƒì„±"""
    print("\n" + "=" * 60)
    print("  Step 4: InferenceService YAML ìƒì„±")
    print("=" * 60)
    
    yaml_content = f"""apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {model_name}
  namespace: {namespace}
  annotations:
    autoscaling.knative.dev/minScale: "1"
    autoscaling.knative.dev/maxScale: "3"
spec:
  predictor:
    sklearn:
      storageUri: "{s3_uri}"
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
"""
    
    yaml_path = f"inference-service-{model_name}.yaml"
    with open(yaml_path, "w") as f:
        f.write(yaml_content)
    
    print(f"\n  âœ… YAML íŒŒì¼ ìƒì„±: {yaml_path}")
    print(f"\n  ë°°í¬ ëª…ë ¹ì–´:")
    print(f"  kubectl apply -f {yaml_path}")
    
    return yaml_path


def main():
    parser = argparse.ArgumentParser(description='KServeìš© ëª¨ë¸ ì¤€ë¹„ ë° S3 ì—…ë¡œë“œ')
    parser.add_argument('--bucket', type=str, default='mlops-training-models',
                        help='S3 ë²„í‚· ì´ë¦„')
    parser.add_argument('--model-name', type=str, default='california-model',
                        help='ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--namespace', type=str, default='kubeflow-user01',
                        help='Kubernetes ë„¤ì„ìŠ¤í˜ì´ìŠ¤')
    parser.add_argument('--skip-upload', action='store_true',
                        help='S3 ì—…ë¡œë“œ ê±´ë„ˆë›°ê¸°')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("  Lab 2-3: KServe ëª¨ë¸ ì¤€ë¹„")
    print("=" * 60)
    print(f"\n  Bucket: {args.bucket}")
    print(f"  Model Name: {args.model_name}")
    print(f"  Namespace: {args.namespace}")
    
    # Step 1: ëª¨ë¸ í•™ìŠµ
    model = train_model()
    
    # Step 2: ë¡œì»¬ ì €ì¥
    model_path = save_model_local(model, output_dir="model")
    
    # Step 3: S3 ì—…ë¡œë“œ
    if not args.skip_upload:
        s3_uri = upload_to_s3(model_path, args.bucket, args.model_name)
        
        if s3_uri:
            # Step 4: YAML ìƒì„±
            generate_inference_yaml(args.model_name, s3_uri, args.namespace)
    else:
        print("\n  â­ï¸  S3 ì—…ë¡œë“œ ê±´ë„ˆëœ€ (--skip-upload)")
        s3_uri = f"s3://{args.bucket}/{args.model_name}/model"
        generate_inference_yaml(args.model_name, s3_uri, args.namespace)
    
    # ì™„ë£Œ
    print("\n" + "=" * 60)
    print("  âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\n  ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. kubectl apply -f inference-service-{args.model_name}.yaml")
    print(f"  2. kubectl get inferenceservice -n {args.namespace}")
    print(f"  3. ./test_inference.sh")
    print("")


if __name__ == '__main__':
    main()
