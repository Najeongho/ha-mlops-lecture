"""
Lab 2-2: MLflow ì‹¤í—˜ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
==================================

ì—¬ëŸ¬ ì‹¤í—˜ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python compare_experiments.py [--experiment california-housing-lab]
"""

import argparse
import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd
import matplotlib.pyplot as plt
import os


def configure_mlflow():
    """MLflow í™˜ê²½ ì„¤ì •"""
    tracking_uri = os.getenv(
        'MLFLOW_TRACKING_URI',
        'http://mlflow-server-service.mlflow-system.svc.cluster.local:5000'
    )
    mlflow.set_tracking_uri(tracking_uri)
    return tracking_uri


def get_experiment_runs(experiment_name: str) -> pd.DataFrame:
    """ì‹¤í—˜ì˜ ëª¨ë“  Runì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    client = MlflowClient()
    
    # ì‹¤í—˜ ID ê°€ì ¸ì˜¤ê¸°
    experiment = client.get_experiment_by_name(experiment_name)
    if experiment is None:
        print(f"âŒ ì‹¤í—˜ '{experiment_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ëª¨ë“  Run ê²€ìƒ‰
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["metrics.r2 DESC"]
    )
    
    return runs


def compare_metrics(runs_df: pd.DataFrame):
    """ë©”íŠ¸ë¦­ ë¹„êµ"""
    print("\n" + "=" * 70)
    print("  ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¹„êµ")
    print("=" * 70)
    
    # í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ
    metric_cols = ['metrics.r2', 'metrics.rmse', 'metrics.mae', 'metrics.mse']
    param_cols = [col for col in runs_df.columns if col.startswith('params.')]
    
    display_cols = ['tags.mlflow.runName'] + metric_cols
    available_cols = [col for col in display_cols if col in runs_df.columns]
    
    if not available_cols:
        print("  âš ï¸  ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ì •ë¦¬
    comparison_df = runs_df[available_cols].copy()
    comparison_df.columns = [col.split('.')[-1] for col in comparison_df.columns]
    
    # ì •ë ¬ ë° ì¶œë ¥
    if 'r2' in comparison_df.columns:
        comparison_df = comparison_df.sort_values('r2', ascending=False)
    
    print("\n  Run Name          R2        RMSE      MAE       MSE")
    print("  " + "-" * 60)
    
    for _, row in comparison_df.iterrows():
        run_name = row.get('runName', 'N/A')[:18].ljust(18)
        r2 = f"{row.get('r2', 0):.4f}".ljust(10) if pd.notna(row.get('r2')) else 'N/A'.ljust(10)
        rmse = f"{row.get('rmse', 0):.4f}".ljust(10) if pd.notna(row.get('rmse')) else 'N/A'.ljust(10)
        mae = f"{row.get('mae', 0):.4f}".ljust(10) if pd.notna(row.get('mae')) else 'N/A'.ljust(10)
        mse = f"{row.get('mse', 0):.4f}" if pd.notna(row.get('mse')) else 'N/A'
        
        print(f"  {run_name}{r2}{rmse}{mae}{mse}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    if 'r2' in comparison_df.columns and len(comparison_df) > 0:
        best_idx = comparison_df['r2'].idxmax()
        best_run = comparison_df.loc[best_idx]
        print("\n  " + "-" * 60)
        print(f"  ğŸ† ìµœê³  ì„±ëŠ¥: {best_run.get('runName', 'N/A')} (R2: {best_run.get('r2', 0):.4f})")
    
    return comparison_df


def plot_comparison(runs_df: pd.DataFrame, output_path: str = "comparison.png"):
    """ë¹„êµ ê·¸ë˜í”„ ìƒì„±"""
    
    # ë°ì´í„° ì¤€ë¹„
    metric_data = []
    for _, row in runs_df.iterrows():
        run_name = row.get('tags.mlflow.runName', 'Unknown')
        r2 = row.get('metrics.r2', None)
        rmse = row.get('metrics.rmse', None)
        
        if pd.notna(r2) and pd.notna(rmse):
            metric_data.append({
                'name': run_name,
                'r2': r2,
                'rmse': rmse
            })
    
    if not metric_data:
        print("  âš ï¸  ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    df = pd.DataFrame(metric_data).sort_values('r2', ascending=True)
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(df)))
    
    # R2 Score
    axes[0].barh(df['name'], df['r2'], color=colors)
    axes[0].set_xlabel('R2 Score', fontsize=12)
    axes[0].set_title('R2 Score Comparison', fontsize=14)
    axes[0].axvline(x=df['r2'].max(), color='red', linestyle='--', alpha=0.7, label='Best')
    axes[0].legend()
    
    # RMSE
    axes[1].barh(df['name'], df['rmse'], color=colors)
    axes[1].set_xlabel('RMSE', fontsize=12)
    axes[1].set_title('RMSE Comparison', fontsize=14)
    axes[1].axvline(x=df['rmse'].min(), color='red', linestyle='--', alpha=0.7, label='Best')
    axes[1].legend()
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\n  ğŸ“ˆ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {output_path}")


def list_experiments():
    """ëª¨ë“  ì‹¤í—˜ ëª©ë¡ ì¶œë ¥"""
    client = MlflowClient()
    experiments = client.search_experiments()
    
    print("\n" + "=" * 60)
    print("  ğŸ“ ë“±ë¡ëœ ì‹¤í—˜ ëª©ë¡")
    print("=" * 60)
    
    for exp in experiments:
        run_count = len(mlflow.search_runs(experiment_ids=[exp.experiment_id]))
        print(f"  - {exp.name} ({run_count} runs)")
    
    print("=" * 60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='MLflow ì‹¤í—˜ ë¹„êµ')
    parser.add_argument('--experiment', type=str, default='california-housing-lab',
                        help='ë¹„êµí•  ì‹¤í—˜ ì´ë¦„')
    parser.add_argument('--list', action='store_true',
                        help='ë“±ë¡ëœ ì‹¤í—˜ ëª©ë¡ ì¶œë ¥')
    parser.add_argument('--output', type=str, default='comparison.png',
                        help='ë¹„êµ ê·¸ë˜í”„ ì¶œë ¥ íŒŒì¼')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("  Lab 2-2: MLflow ì‹¤í—˜ ë¹„êµ")
    print("=" * 60)
    
    # MLflow ì„¤ì •
    tracking_uri = configure_mlflow()
    print(f"\n  MLflow Tracking URI: {tracking_uri}")
    
    # ì‹¤í—˜ ëª©ë¡ ì¶œë ¥
    if args.list:
        list_experiments()
        return
    
    # ì‹¤í—˜ Run ê°€ì ¸ì˜¤ê¸°
    print(f"\n  ì‹¤í—˜: {args.experiment}")
    runs_df = get_experiment_runs(args.experiment)
    
    if runs_df.empty:
        print("  âš ï¸  ì‹¤í—˜ì— Runì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"  ì´ {len(runs_df)}ê°œì˜ Run ë°œê²¬")
    
    # ë©”íŠ¸ë¦­ ë¹„êµ
    compare_metrics(runs_df)
    
    # ì‹œê°í™”
    try:
        import numpy as np
        plot_comparison(runs_df, args.output)
    except ImportError:
        print("\n  âš ï¸  matplotlibê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install matplotlib")
    
    print("\n" + "=" * 60)
    print("  âœ… ë¹„êµ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == '__main__':
    import numpy as np  # plot_comparisonì—ì„œ ì‚¬ìš©
    main()
